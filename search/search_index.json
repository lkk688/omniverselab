{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Omniverse Learning Lab","text":"<p>Welcome to the Omniverse Learning Lab! This comprehensive repository provides hands-on tutorials, documentation, and practical examples for learning NVIDIA Omniverse, OpenUSD, Isaac Sim, and related technologies for 3D simulation, autonomous systems, and collaborative development.</p>"},{"location":"#documentation-overview","title":"\ud83d\udcda Documentation Overview","text":""},{"location":"#openusd-foundations","title":"OpenUSD Foundations","text":"<ul> <li>OpenUSD Foundations - Core concepts, stages, prims, attributes, and fundamental USD operations</li> <li>OpenUSD Applied - Advanced techniques, composition arcs, layer management, and practical applications</li> </ul>"},{"location":"#omniverse-platform","title":"Omniverse Platform","text":"<ul> <li>Omniverse Kit - Kit SDK development, extensions, and application building</li> <li>Kit App Streaming - Cloud streaming, deployment, and remote access solutions</li> </ul>"},{"location":"#simulation-robotics","title":"Simulation &amp; Robotics","text":"<ul> <li>Isaac Sim - Robotics simulation, physics, sensors, and ROS integration</li> <li>ROS2 Integration - Connecting Isaac Sim with ROS2 ecosystems</li> <li>Hardware-in-the-Loop Simulation - Real-world hardware integration with simulation</li> <li>Embodied AI - Robotics simulation, physics, sensors, and ROS integration</li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA GPU with RTX capabilities</li> <li>Ubuntu 20.04+ or Windows 10/11</li> <li>Python 3.7+</li> <li>Basic understanding of 3D graphics and simulation concepts</li> </ul>"},{"location":"#quick-start-guide","title":"Quick Start Guide","text":"<ol> <li>OpenUSD Basics: Start with OpenUSD Foundations to understand core concepts</li> <li>Platform Setup: Follow Omniverse Kit for development environment setup</li> <li>Simulation: Explore Isaac Sim for robotics and autonomous systems</li> <li>Integration: Connect with ROS2 Integration for real-world applications</li> </ol>"},{"location":"#practical-examples","title":"\ud83d\udee0\ufe0f Practical Examples","text":""},{"location":"#isaac-sim-projects","title":"Isaac Sim Projects","text":"<ul> <li>Autonomous Driving Simulation (<code>IsaacSim/autonomous_driving_isaac_sim.py</code>)</li> <li>System Health Checks (<code>IsaacSim/vm_system_check.py</code>)</li> <li>ROS2 Setup Validation (<code>IsaacSim/isaac_ros2_setup_check.py</code>)</li> </ul>"},{"location":"#openusd-notebooks","title":"OpenUSD Notebooks","text":"<ul> <li>Interactive Jupyter notebooks in <code>openusd/</code> directory</li> <li>Hands-on exercises for USD fundamentals</li> <li>Advanced composition and animation examples</li> </ul>"},{"location":"#learning-paths","title":"\ud83c\udfaf Learning Paths","text":""},{"location":"#for-beginners","title":"For Beginners","text":"<ol> <li>OpenUSD Foundations \u2192 OpenUSD Applied</li> <li>Omniverse Kit \u2192 Basic extension development</li> <li>Isaac Sim \u2192 Simple robot simulation</li> </ol>"},{"location":"#for-developers","title":"For Developers","text":"<ol> <li>Omniverse Kit \u2192 Kit App Streaming</li> <li>Advanced USD composition and workflows</li> <li>Custom extension and application development</li> </ol>"},{"location":"#for-researchers","title":"For Researchers","text":"<ol> <li>Isaac Sim \u2192 ROS2 Integration</li> <li>Hardware-in-the-Loop Simulation</li> </ol>"},{"location":"#tools-utilities","title":"\ud83d\udd27 Tools &amp; Utilities","text":"<ul> <li>System Checks: Automated validation scripts for Isaac Sim and ROS2 setup</li> <li>USD Utilities: Helper functions for USD file manipulation and visualization</li> <li>Simulation Assets: Pre-configured environments and vehicle models</li> </ul>"},{"location":"#additional-resources","title":"\ud83d\udcd6 Additional Resources","text":"<ul> <li>Images &amp; Diagrams: Visual aids in <code>docs/imgs/</code> for better understanding</li> <li>Asset Library: USD files and 3D models in <code>openusd/assets/</code></li> <li>Code Examples: Practical implementations across all directories</li> </ul>"},{"location":"cmpe249_isaac_sim_reflection/","title":"CMPE249 Intelligent Autonomous Systems - Isaac Sim Training Reflection","text":""},{"location":"cmpe249_isaac_sim_reflection/#training-completion-statement","title":"Training Completion Statement","text":"<p>I have successfully completed the Nvidia Omniverse training. This comprehensive training covered the fundamentals of OpenUSD foundations, OpenUSD modular architecture principles, Omniverse Kit environment setup, ISSAC Sim setup, physics configuration, sensor simulation (cameras, LiDAR), data collection and hardware in the loop simulation, through hands-on experience with Omniverse Kit development workflows. I have created a github repository (https://github.com/lkk688/omniverselab) with Github hosted documents (https://lkk688.github.io/omniverselab/) to document my learning process and share my additional code with the community.</p>"},{"location":"cmpe249_isaac_sim_reflection/#key-takeaway-from-training-experience","title":"Key Takeaway from Training Experience","text":"<p>The most significant insight was understanding how modular architecture and OpenUSD foundations create powerful simulation platforms. Through comprehensive study of OpenUSD fundamentals, applied OpenUSD techniques, and Omniverse Kit development, I learned that well-structured simulation frameworks enable:</p> <ul> <li>Reusable components across different scenarios</li> <li>Robust error handling and system integration</li> <li>Seamless collaboration between research teams</li> </ul> <p>This foundation transforms Isaac Sim from a simulation tool into a comprehensive platform for autonomous systems research.</p> <p>I plan to extend the current Isaac Sim sensor simulation capabilities to create a comprehensive multi-modal sensor fusion testbed for the CMPE249 Intelligent Autonomous Systems class:</p>"},{"location":"cmpe249_isaac_sim_reflection/#1-advanced-sensor-integration","title":"1. Advanced Sensor Integration","text":"<p>Development of a comprehensive multi-modal sensor suite including RGB cameras, depth cameras, LiDAR arrays, automotive radar, IMU sensors, and GPS simulation. This framework will enable real-time sensor data fusion for autonomous decision making and provide students with hands-on experience with industry-standard sensor configurations.</p>"},{"location":"cmpe249_isaac_sim_reflection/#2-intelligent-scenario-generation","title":"2. Intelligent Scenario Generation","text":"<ul> <li>Dynamic Weather Conditions: Rain, fog, snow effects on sensor performance</li> <li>Complex Traffic Scenarios: Multi-agent interactions, emergency situations</li> <li>Edge Case Testing: Sensor failure modes, occlusion handling</li> <li>Real-world Data Integration: Import actual traffic patterns and road conditions</li> </ul>"},{"location":"cmpe249_isaac_sim_reflection/#3-educational-integration","title":"3. Educational Integration","text":"<p>Creation of a structured assignment framework that covers four key areas: perception (object detection and tracking), planning (path planning in dynamic environments), control (vehicle dynamics and control systems), and fusion (multi-sensor data fusion algorithms). Each student will have access to isolated simulation environments tailored to specific learning objectives.</p>"},{"location":"cmpe249_isaac_sim_reflection/#4-research-applications","title":"4. Research Applications","text":"<p>The multi-modal sensor fusion testbed based on NVIDIA Isaac Sim opens exciting research opportunities for advancing autonomous systems:</p> <ul> <li>Multi-Modal Dataset Generation: Create comprehensive datasets combining camera, LiDAR, radar, and IMU data with perfect ground truth annotations for training next-generation multi-modal large models</li> <li>Sensor Fusion Algorithm Development: Test and validate novel sensor fusion approaches in controlled, repeatable environments with known ground truth</li> <li>Edge Case Simulation: Generate rare driving scenarios and adverse weather conditions that are difficult or dangerous to collect in real-world testing</li> <li>Synthetic-to-Real Transfer Learning: Investigate domain adaptation techniques to bridge the gap between simulated and real-world sensor data</li> <li>Perception Model Benchmarking: Establish standardized evaluation protocols for autonomous driving perception systems across different sensor modalities</li> <li>Human-Robot Interaction Studies: Simulate complex urban environments with pedestrians and cyclists to study interaction patterns and safety protocols</li> </ul>"},{"location":"cmpe249_isaac_sim_reflection/#implementation-timeline","title":"Implementation Timeline","text":"Phase Duration Deliverables Phase 1 2 weeks Enhanced sensor suite implementation Phase 2 3 weeks Dynamic scenario generation system Phase 3 2 weeks Student assignment framework Phase 4 1 week Documentation and testing"},{"location":"cmpe249_isaac_sim_reflection/#expected-outcomes","title":"Expected Outcomes","text":"<ol> <li>Enhanced Learning Experience: Students will work with industry-standard simulation tools</li> <li>Research Advancement: Novel approaches to autonomous systems testing and validation</li> <li>Industry Relevance: Direct application to real-world autonomous vehicle development</li> <li>Collaborative Opportunities: Foundation for interdisciplinary research projects</li> </ol>"},{"location":"cmpe249_isaac_sim_reflection/#repository-structure","title":"Repository Structure","text":"<p>The GitHub repository is organized with the core Isaac Sim framework in the <code>IsaacSim/</code> directory, including the main simulation script and comprehensive tutorial. Class-specific extensions will be housed in <code>cmpe249_extensions/</code> containing advanced sensor implementations, scenario generators, and assignment frameworks. Documentation resides in <code>docs/</code> with this reflection and additional guides, while <code>examples/</code> provides student assignments and research scenarios for practical application.</p>"},{"location":"cmpe249_isaac_sim_reflection/#conclusion","title":"Conclusion","text":"<p>This Isaac Sim training has provided a solid foundation for advancing autonomous systems education and research. The modular architecture and comprehensive sensor simulation capabilities will enable innovative teaching methodologies and cutting-edge research opportunities in the CMPE249 Intelligent Autonomous Systems class.</p> <p>The planned extensions will not only enhance student learning but also contribute to the broader autonomous systems research community through novel simulation frameworks and interdisciplinary collaboration opportunities.</p> <p>Document created as part of CMPE249 Intelligent Autonomous Systems coursework Author: [Your Name] Date: [Current Date] Course: CMPE249 - Intelligent Autonomous Systems</p>"},{"location":"hil_sim/","title":"HIL Simulation","text":"<p>Integrate real hardware like Jetson.</p>"},{"location":"isaac_sim/","title":"Isaac Sim: Comprehensive Robotics Simulation Platform","text":"<p>NVIDIA Isaac Sim is a powerful robotics simulation platform built on NVIDIA Omniverse, designed for developing, testing, and training AI-powered robots. It provides photorealistic simulation environments with accurate physics, enabling seamless integration with ROS/ROS2 ecosystems and real-world deployment workflows.</p>"},{"location":"isaac_sim/#learning-resources","title":"\ud83d\udcda Learning Resources","text":""},{"location":"isaac_sim/#official-nvidia-courses","title":"Official NVIDIA Courses","text":"<ul> <li>Course 1: Getting Started: Simulating Your First Robot in Isaac Sim</li> <li>Course 2: Ingesting Robot Assets and Simulating Your Robot in Isaac Sim</li> <li>Course 3: Synthetic Data Generation for Perception Model Training in Isaac Sim</li> <li>Course 4: Developing Robots With Software-in-the-Loop (SIL) In Isaac Sim</li> <li>Course 5: Leveraging ROS 2 and Hardware-in-the-Loop (HIL) in Isaac Sim</li> </ul>"},{"location":"isaac_sim/#key-documentation","title":"Key Documentation","text":"<ul> <li>Isaac Sim Official Documentation</li> <li>Isaac ROS Documentation</li> <li>Omniverse Platform Documentation</li> <li>USD (Universal Scene Description) Documentation</li> </ul>"},{"location":"isaac_sim/#getting-started-course-1","title":"\ud83d\ude80 Getting Started (Course 1)","text":""},{"location":"isaac_sim/#installation-and-setup","title":"Installation and Setup","text":"<p>Official Installation Guide: Isaac Sim Workstation Installation</p>"},{"location":"isaac_sim/#command-line-launch","title":"Command Line Launch","text":"<p>The Isaac Sim app can be run directly from the command line: - Linux: <code>./isaac-sim.sh</code> - Windows: <code>isaac-sim.bat</code></p>"},{"location":"isaac_sim/#post-installation-setup","title":"Post-Installation Setup","text":"<p>To create symlinks for extension examples and tutorials: - Linux: <code>./post_install.sh</code> - Windows: Double-click <code>post_install.bat</code></p>"},{"location":"isaac_sim/#isaac-sim-app-selector","title":"Isaac Sim App Selector","text":"<p>Launch the app selector to choose your Isaac Sim configuration: - Linux: <code>./isaac-sim.selector.sh</code> - Windows: Double-click <code>isaac-sim.selector.bat</code> - Select Isaac Sim Full in the popup window</p>"},{"location":"isaac_sim/#starting-isaac-sim","title":"Starting Isaac Sim","text":"<ol> <li>Navigate to your Isaac Sim installation directory:    <code>bash    cd ~/isaac-sim</code></li> <li>Launch the selector:    <code>bash    ./isaac-sim.selector.sh</code></li> <li>Note: For basic tutorials, ensure ROS Bridge extensions are initially disabled</li> </ol>"},{"location":"isaac_sim/#viewport-navigation","title":"Viewport Navigation","text":"<p>Camera Controls: - Movement: Right mouse + WASD keys (forward/backward/left/right) - Vertical: Right mouse + Q/E keys (up/down) - Rotation: Right mouse + drag - Zoom: Mouse wheel or Alt + right mouse - Panning: Middle mouse + drag</p>"},{"location":"isaac_sim/#ros2-integration","title":"\ud83e\udd16 ROS2 Integration","text":""},{"location":"isaac_sim/#prerequisites","title":"Prerequisites","text":"<ul> <li>ROS2 Documentation: ROS2 Humble</li> <li>Isaac ROS: NVIDIA Isaac ROS</li> </ul>"},{"location":"isaac_sim/#ros2-setup-for-isaac-sim","title":"ROS2 Setup for Isaac Sim","text":"<ol> <li>Install Required Packages:    ```bash    # Install vision_msgs package (required for ROS2 Bridge)    sudo apt install ros-humble-vision-msgs</li> </ol> <p># Install additional Isaac ROS packages    sudo apt install ros-humble-isaac-ros-*    ```</p> <ol> <li>Environment Configuration:    ```bash    # Source ROS2 workspace    source /opt/ros/humble/setup.bash</li> </ol> <p># Verify ROS2 installation    ros2 topic list    ```</p> <ol> <li>Launch Isaac Sim with ROS2:    <code>bash    # Ensure ROS2 is sourced before launching    ./isaac-sim.sh</code></li> </ol>"},{"location":"isaac_sim/#isaac-ros-integration","title":"Isaac ROS Integration","text":"<p>Isaac ROS provides GPU-accelerated ROS2 packages for: - Perception: Computer vision and AI inference - Navigation: Path planning and obstacle avoidance - Manipulation: Robotic arm control - Simulation: Isaac Sim integration</p> <p>Key Isaac ROS Packages: - <code>isaac_ros_visual_slam</code>: Real-time SLAM - <code>isaac_ros_nvblox</code>: 3D scene reconstruction - <code>isaac_ros_apriltag</code>: AprilTag detection - <code>isaac_ros_dnn_inference</code>: AI model inference</p> <p>Installation:</p> <pre><code># Install Isaac ROS (requires NVIDIA GPU)\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\ncd isaac_ros_common &amp;&amp; ./scripts/run_dev.sh\n</code></pre> <p>References: - Isaac ROS Developer Guide - Isaac ROS Packages - Isaac Sim ROS2 Bridge</p>"},{"location":"isaac_sim/#robot-asset-integration-course-2","title":"\ud83e\udd16 Robot Asset Integration (Course 2)","text":""},{"location":"isaac_sim/#understanding-robot-description-formats","title":"Understanding Robot Description Formats","text":"<p>URDF (Unified Robot Description Format) is an XML-based standard for describing robot configurations: - Links: Physical components (chassis, wheels, sensors) - Joints: Connections between links (revolute, prismatic, fixed) - Geometry: Visual and collision meshes - Physics: Mass, inertia, friction properties - Sensors: Camera, LiDAR, IMU specifications</p>"},{"location":"isaac_sim/#importing-robot-assets","title":"Importing Robot Assets","text":""},{"location":"isaac_sim/#urdf-import-workflow","title":"URDF Import Workflow","text":"<ol> <li> <p>Launch Isaac Sim:    <code>bash    ./isaac-sim.sh</code></p> </li> <li> <p>Access URDF Importer:</p> </li> <li>Navigate to <code>Isaac Utils \u2192 Workflows \u2192 URDF Importer</code></li> <li> <p>Select input file (e.g., Carter Robot: <code>carter.urdf</code>)</p> </li> <li> <p>Locate Built-in Assets:</p> </li> <li>Check Bookmarks folder for \"Built-In URDF Files\"</li> <li>Download additional assets from Isaac Sim Assets</li> </ol>"},{"location":"isaac_sim/#supported-asset-formats","title":"Supported Asset Formats","text":"<ul> <li>URDF: ROS standard robot description</li> <li>USD: Universal Scene Description (native Omniverse)</li> <li>MJCF: MuJoCo XML format</li> <li>SDF: Simulation Description Format (Gazebo)</li> </ul> <p>Asset Conversion Tools: - URDF to USD Converter - Isaac Sim Asset Converter</p>"},{"location":"isaac_sim/#robot-configuration","title":"Robot Configuration","text":""},{"location":"isaac_sim/#link-hierarchy","title":"Link Hierarchy","text":"<ul> <li>Root Link: <code>chassis_link</code> serves as the robot's base</li> <li>Child Links: All other components (wheels, sensors) inherit from the root</li> <li>Transform Chain: Defines spatial relationships between components</li> </ul>"},{"location":"isaac_sim/#base-link-configuration","title":"Base Link Configuration","text":"<p>Fix Base Link Setting: - \u2705 Checked: For manipulator arms (stationary base) - \u274c Unchecked: For mobile robots (movable base)</p>"},{"location":"isaac_sim/#carter-robot-setup","title":"Carter Robot Setup","text":"<p>Carter Robot Specifications: - Type: Two-wheeled differential drive mobile robot - Configuration: Active front wheels + passive rear caster - Control: Velocity-based wheel control</p>"},{"location":"isaac_sim/#joint-configuration","title":"Joint Configuration","text":"Joint Type Target Type Purpose <code>left_wheel</code> Revolute Velocity Active drive wheel <code>right_wheel</code> Revolute Velocity Active drive wheel <code>rear_axle</code> Revolute None Passive caster support <code>rear_pivot</code> Revolute None Passive caster rotation <p>Configuration Steps: 1. Set <code>left_wheel</code> and <code>right_wheel</code> target types to Velocity 2. Set <code>rear_axle</code> and <code>rear_pivot</code> target types to None 3. Configure wheel parameters (radius, separation distance)</p> <p>Why Velocity Control? - Position Control: Suitable for precise positioning (manipulators) - Velocity Control: Ideal for continuous motion (mobile robots) - Passive Joints: No motor control, move naturally with robot motion</p>"},{"location":"isaac_sim/#scene-management","title":"Scene Management","text":""},{"location":"isaac_sim/#stage-window","title":"Stage Window","text":"<p>The Stage Window is Isaac Sim's scene hierarchy manager: - USD Scene Graph: Displays all scene elements in tree structure - Asset Organization: Shows imported robots, environments, and props - Transform Hierarchy: Visualizes parent-child relationships - Property Access: Right-click for context menus and properties</p> <p>URDF to USD Conversion: - Automatic conversion during import - Preserves link relationships and joint definitions - Maintains physics and visual properties</p>"},{"location":"isaac_sim/#environment-setup","title":"Environment Setup","text":""},{"location":"isaac_sim/#adding-ground-plane","title":"Adding Ground Plane","text":"<ol> <li>Create Environment:    <code>Create \u2192 Environments \u2192 Flat Grid</code></li> <li>Physics Requirement: Ground plane essential for:</li> <li>Gravity simulation</li> <li>Collision detection</li> <li>Realistic robot movement</li> </ol>"},{"location":"isaac_sim/#lighting-configuration","title":"Lighting Configuration","text":"<p>Environment Light Management: - Default Light: Often too bright for robot visualization - Toggle Visibility: Click \"eye\" icon in Stage window - Custom Lighting: Add directional or point lights as needed</p>"},{"location":"isaac_sim/#robot-positioning","title":"Robot Positioning","text":"<p>Initial Placement: 1. Check Position: Robot may spawn below ground level 2. Adjust Z-Axis: Move robot above ground plane 3. Methods:    - Viewport: Drag robot vertically    - Transform Panel: Set precise Z-coordinate    - Properties: Modify translation values</p>"},{"location":"isaac_sim/#visualization-and-physics","title":"Visualization and Physics","text":""},{"location":"isaac_sim/#collision-visualization","title":"Collision Visualization","text":"<p>Show By Type Controls: 1. Click \"eye\" icon at viewport top 2. Navigate to <code>Physics \u2192 Colliders</code> 3. Select None to hide collision meshes 4. Improves visual clarity during setup</p>"},{"location":"isaac_sim/#joint-analysis","title":"Joint Analysis","text":"<p>Carter Joint Inspection: - Active Joints: <code>left_wheel</code>, <code>right_wheel</code> (with damping) - Passive Joints: <code>rear_pivot</code>, <code>rear_axle</code> (no damping) - Joint Properties: Accessible via Stage window expansion</p> <p>Manual Wheel Control:</p> <pre><code>1. Select both wheel joints in Stage window\n2. Set Target Velocity = 20 (forward motion)\n3. Observe robot movement behavior\n</code></pre>"},{"location":"isaac_sim/#differential-drive-controller","title":"Differential Drive Controller","text":""},{"location":"isaac_sim/#controller-overview","title":"Controller Overview","text":"<p>Differential Drive Kinematics: - Forward/Backward: Both wheels same direction - Turning: Wheels opposite directions or different speeds - Spot Turn: Wheels equal but opposite velocities</p>"},{"location":"isaac_sim/#omnigraph-integration","title":"OmniGraph Integration","text":"<p>Setup Workflow: 1. Access Controller:    <code>Tools \u2192 Robotics \u2192 OmniGraph Controllers \u2192 Differential Controller</code> 2. Pre-built Graph: Automatic OmniGraph creation 3. Parameter Configuration: Wheel specifications required</p>"},{"location":"isaac_sim/#controller-parameters","title":"Controller Parameters","text":"<p>Wheel Radius Calculation: 1. Select collision geometry in <code>left_wheel_link</code> or <code>right_wheel_link</code> 2. Check cylinder radius in properties: 0.24 meters 3. Enter value in Differential Controller settings</p> <p>Wheel Separation Distance: 1. Check <code>left_wheel_link</code> Y-position: +0.31 meters 2. Check <code>right_wheel_link</code> Y-position: -0.31 meters 3. Calculate separation: <code>0.31 \u00d7 2 = 0.62 meters</code> 4. Enter value in Wheel Distance field</p> <p>Control Configuration: - Keyboard Control: Enable for WASD movement - Speed Limits:   - <code>maxLinearSpeed</code>: 0.2 m/s (reasonable for testing)   - <code>maxAngularSpeed</code>: 0.2 rad/s (safe turning speed)</p> <p>Control Mapping: - W: Forward - S: Backward - A: Turn Left - D: Turn Right</p>"},{"location":"isaac_sim/#sensor-integration-isaac-ros","title":"\ud83d\udd0d Sensor Integration &amp; Isaac ROS","text":""},{"location":"isaac_sim/#sensor-suite-overview","title":"Sensor Suite Overview","text":""},{"location":"isaac_sim/#available-sensors-in-isaac-sim","title":"Available Sensors in Isaac Sim","text":"<p>Vision Sensors: - RGB Camera: Standard color imaging - Depth Camera: Distance measurement - Stereo Camera: Binocular depth perception - Fisheye Camera: Wide-angle imaging - Semantic Segmentation: Object classification - Instance Segmentation: Individual object identification</p> <p>LiDAR Sensors: - Rotating LiDAR: 360\u00b0 scanning (Velodyne-style) - Solid State LiDAR: Fixed scanning pattern - 2D LiDAR: Planar scanning for navigation</p> <p>IMU &amp; Navigation: - IMU Sensor: Acceleration and angular velocity - Contact Sensor: Collision detection - Effort Sensor: Joint force/torque measurement</p>"},{"location":"isaac_sim/#camera-integration","title":"Camera Integration","text":""},{"location":"isaac_sim/#adding-rgb-camera","title":"Adding RGB Camera","text":"<p>Setup Process: 1. Create Camera:    <code>Create \u2192 Camera</code> 2. Position Camera: Attach to robot or place in scene 3. Configure Properties:    - Resolution: 1920\u00d71080 (adjustable)    - FOV: 60\u00b0 (horizontal field of view)    - Clipping: Near/far plane distances</p>"},{"location":"isaac_sim/#ros2-camera-publisher","title":"ROS2 Camera Publisher","text":"<p>OmniGraph Setup: 1. Access Graph Editor:    <code>Window \u2192 Visual Scripting \u2192 Action Graph</code> 2. Add Camera Nodes:    - <code>Isaac Read Camera Info</code>    - <code>ROS2 Camera Helper</code>    - <code>ROS2 Publish Image</code></p> <p>Camera Topics: - <code>/camera/image_raw</code>: Raw RGB images - <code>/camera/camera_info</code>: Camera calibration - <code>/camera/depth</code>: Depth information (if enabled)</p>"},{"location":"isaac_sim/#lidar-integration","title":"LiDAR Integration","text":""},{"location":"isaac_sim/#rotating-lidar-setup","title":"Rotating LiDAR Setup","text":"<p>Creation Workflow: 1. Add LiDAR:    <code>Create \u2192 Isaac \u2192 Sensors \u2192 Rotating Lidar</code> 2. Configure Parameters:    - Horizontal Resolution: 0.4\u00b0 (900 points)    - Vertical Resolution: 26.8\u00b0 (64 channels)    - Range: 0.4m to 100m    - Rotation Frequency: 20 Hz</p>"},{"location":"isaac_sim/#ros2-lidar-publisher","title":"ROS2 LiDAR Publisher","text":"<p>OmniGraph Configuration: - Node: <code>Isaac Read Lidar Point Cloud</code> - Publisher: <code>ROS2 Publish Point Cloud</code> - Topic: <code>/scan</code> or <code>/velodyne_points</code> - Frame: <code>lidar_link</code></p>"},{"location":"isaac_sim/#isaac-ros-integration_1","title":"Isaac ROS Integration","text":""},{"location":"isaac_sim/#isaac-ros-packages","title":"Isaac ROS Packages","text":"<p>Core Packages: - isaac_ros_visual_slam: Visual-inertial SLAM - isaac_ros_nvblox: 3D reconstruction and mapping - isaac_ros_apriltag: Fiducial marker detection - isaac_ros_object_detection: AI-powered object detection - isaac_ros_depth_segmentation: Depth-based segmentation - isaac_ros_stereo_image_proc: Stereo vision processing</p>"},{"location":"isaac_sim/#vslam-integration","title":"VSLAM Integration","text":"<p>Setup Requirements: 1. Install Isaac ROS VSLAM:    <code>bash    sudo apt install ros-humble-isaac-ros-visual-slam</code></p> <ol> <li>Camera Configuration:</li> <li>Stereo cameras or RGB-D camera</li> <li>IMU sensor for enhanced accuracy</li> <li> <p>Proper calibration essential</p> </li> <li> <p>Launch VSLAM:    <code>bash    ros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py</code></p> </li> </ol>"},{"location":"isaac_sim/#nvblox-mapping","title":"Nvblox Mapping","text":"<p>3D Reconstruction Pipeline: 1. Input Sources:    - RGB-D camera streams    - Pose estimates from VSLAM    - Optional LiDAR for enhanced accuracy</p> <ol> <li>Output Products:</li> <li>TSDF Volume: Truncated Signed Distance Field</li> <li>Mesh: 3D surface reconstruction</li> <li> <p>Occupancy Grid: 2D navigation map</p> </li> <li> <p>Integration Command:    <code>bash    ros2 launch isaac_ros_nvblox nvblox_isaac_sim.launch.py</code></p> </li> </ol>"},{"location":"isaac_sim/#sensor-fusion-workflows","title":"Sensor Fusion Workflows","text":""},{"location":"isaac_sim/#multi-modal-perception","title":"Multi-Modal Perception","text":"<p>Camera + LiDAR Fusion: - Object Detection: 2D bounding boxes + 3D point clouds - Semantic Mapping: Color + geometry information - Obstacle Avoidance: Dense depth + sparse LiDAR</p> <p>IMU + Vision Integration: - Visual-Inertial Odometry: Robust pose estimation - Motion Compensation: Stabilized imaging - Dynamic Object Tracking: Motion-aware detection</p>"},{"location":"isaac_sim/#isaac-ros-perception-pipeline","title":"Isaac ROS Perception Pipeline","text":"<p>Complete Workflow:</p> <pre><code># 1. Launch Isaac Sim with sensors\n# 2. Start ROS2 bridge\nros2 launch isaac_ros_launch isaac_ros_dev.launch.py\n\n# 3. Run perception stack\nros2 launch isaac_ros_object_detection isaac_ros_detectnet.launch.py\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py\nros2 launch isaac_ros_nvblox nvblox_isaac_sim.launch.py\n</code></pre> <p>Sensors like cameras and Lidar need to be added manually in Isaac Sim. Carter has designated spots for these sensors: At the front, there\u2019s a rounded rectangle designed for a stereo camera. On the top, there\u2019s a cylinder meant for a 2D lidar.</p> <p>Navigate to Create &gt; Camera in the menu. This adds a new camera to your scene. Attach the Camera to Carter In the Stage window, drag and drop the camera under Carter &gt; Chassis_link. This ensures that the camera moves with the robot during simulation. Double-click the camera in the stage to rename it to RGB_Sensor so it\u2019s clear what this object represents.</p> <p>Change your viewport from Perspective to RGB_Sensor. You can do this by selecting the camera dropdown in the top of your Viewport, then selecting Cameras &gt; RGB_Sensor.</p> <p>Navigate to Create &gt; Isaac &gt; Sensors &gt; PhysX Lidar &gt; Rotating. Just like with the camera, drag and drop this lidar sensor under Carter &gt; Chassis_link in the Stage window.</p> <p>Select the Lidar sensor in the Stage window. Scroll down to its Raw USD Properties in the Property panel. Enable Draw Lines. This will allow you to see lidar beams during simulation: Gray beams indicate areas where no objects are detected. Red beams indicate that an object has been hit by a beam (e.g., walls or obstacles).</p> <p>Add Obstacles: Navigate to Create &gt; Mesh &gt; Cube (or another primitive like a sphere or cylinder) to add a basic shape to your stage. Place these meshes around the flat grid so they act as obstacles for Carter to detect.</p> <p>Nova Carter is a next-generation Autonomous Mobile Robot (AMR) platform powered by NVIDIA\u2019s Jetson AGX Orin architecture. Unlike the Carter robot we imported using the URDF Importer, Nova Carter comes fully pre-configured with advanced sensors, materials, and physics properties, making it ready to use out of the box. This robot is ideal for tasks like 3D mapping, navigation, and perception-based AI development. In the Stage window, expand chassis_link under Nova Carter to explore its components. You\u2019ll notice that Nova Carter includes an array of sensors that are already set up and ready for use.</p>"},{"location":"isaac_sim/#synthetic-data-generation-course-3","title":"\ud83d\udcca Synthetic Data Generation (Course 3)","text":""},{"location":"isaac_sim/#overview-ai-powered-data-creation","title":"Overview: AI-Powered Data Creation","text":"<p>Synthetic Data Generation revolutionizes machine learning by creating photorealistic, annotated datasets without real-world data collection. Isaac Sim's NVIDIA Replicator provides enterprise-grade synthetic data generation capabilities.</p>"},{"location":"isaac_sim/#why-synthetic-data","title":"Why Synthetic Data?","text":"<p>Advantages: - Cost Effective: No physical data collection required - Scalable: Generate millions of samples automatically - Perfect Annotations: Ground truth labels without human error - Rare Scenarios: Create edge cases and dangerous situations safely - Domain Randomization: Improve model generalization</p>"},{"location":"isaac_sim/#nvidia-replicator-framework","title":"NVIDIA Replicator Framework","text":""},{"location":"isaac_sim/#quick-start-warehouse-dataset","title":"Quick Start: Warehouse Dataset","text":"<p>Clone Training Workflow:</p> <pre><code>git clone https://github.com/NVIDIA-AI-IOT/synthetic_data_generation_training_workflow.git\ncd synthetic_data_generation_training_workflow/local\n</code></pre> <p>Configure Isaac Sim Path:</p> <pre><code># Edit generate_data.sh\nexport ISAAC_SIM_PATH=\"/path/to/isaac-sim\"\n./generate_data.sh\n</code></pre>"},{"location":"isaac_sim/#core-replicator-components","title":"Core Replicator Components","text":"<p>Environment Setup:</p> <pre><code># Load warehouse environment\nENV_URL = \"/Isaac/Environments/Simple_Warehouse/warehouse.usd\"\nopen_stage(prefix_with_isaac_asset_server(ENV_URL))\n\n# Add pallet jacks from SimReady assets\npallet_jacks = rep.create.from_usd(\n    \"/Isaac/Props/Palletjack/palletjack.usd\",\n    count=10\n)\n</code></pre> <p>Domain Randomization:</p> <pre><code>with rep.trigger.on_frame(num_frames=1000):\n    # Randomize object poses\n    rep.modify.pose(\n        pallet_jacks,\n        position=rep.distribution.uniform(\n            (-10, 0, -10), (10, 0, 10)\n        )\n    )\n\n    # Randomize materials and lighting\n    rep.randomizer.materials(pallet_jacks)\n    rep.modify.attribute(\n        \"Dome_Light\",\n        intensity=rep.distribution.uniform(500, 2000)\n    )\n</code></pre>"},{"location":"isaac_sim/#machine-learning-integration","title":"Machine Learning Integration","text":""},{"location":"isaac_sim/#detectnet_v2-training-pipeline","title":"DetectNet_v2 Training Pipeline","text":"<p>Model Architecture: ResNet-based object detection optimized for synthetic data</p> <p>Training Workflow: 1. Setup Environment:    ```bash    # Clone training repository    git clone https://github.com/NVIDIA-AI-IOT/synthetic_data_generation_training_workflow.git</p> <p># Launch Jupyter notebook    jupyter notebook local_train.ipynb    ```</p> <ol> <li>TAO Toolkit Integration:    ```bash    # Setup TAO via Docker    docker pull nvcr.io/nvidia/tao/tao-toolkit:5.0.0-tf2.11.0</li> </ol> <p># Convert to TFRecords    tao dataset_convert -d /data/synthetic -o /data/tfrecords    ```</p> <ol> <li>Model Training:    ```python    # Configure training parameters    training_config = {        \"batch_size\": 16,        \"learning_rate\": 0.0001,        \"epochs\": 100,        \"augmentation\": True    }</li> </ol> <p># Train DetectNet_v2    tao detectnet_v2 train -e /config/detectnet_v2.yaml    ```</p>"},{"location":"isaac_sim/#isaac-ros-deployment","title":"Isaac ROS Deployment","text":"<p>Real-time Inference Pipeline:</p> <pre><code># Deploy trained model with Isaac ROS\nros2 launch isaac_ros_object_detection isaac_ros_detectnet.launch.py \\\n    model_file_path:=/models/detectnet_v2.engine \\\n    engine_file_path:=/models/detectnet_v2.engine\n\n# Stream synthetic data for testing\nros2 run isaac_ros_replicator synthetic_data_publisher\n</code></pre>"},{"location":"isaac_sim/#advanced-applications","title":"Advanced Applications","text":""},{"location":"isaac_sim/#multi-domain-training","title":"Multi-Domain Training","text":"<p>Logistics Scene Understanding: - LOCO Dataset Integration: Logistics scene dataset - Warehouse Operations: Pallet detection, forklift navigation - Industrial Inspection: Quality control, safety monitoring</p>"},{"location":"isaac_sim/#jetson-deployment","title":"Jetson Deployment","text":"<p>Edge AI Integration: - Isaac ROS: GPU-accelerated ROS 2 packages - DeepStream SDK: Real-time video analytics - TensorRT Optimization: Inference acceleration</p> <p>Deployment Command:</p> <pre><code># Deploy on Jetson with Isaac ROS\nros2 launch isaac_ros_launch isaac_ros_dev.launch.py \\\n    camera:=realsense \\\n    model:=detectnet_v2\n</code></pre>"},{"location":"isaac_sim/#performance-optimization","title":"Performance Optimization","text":""},{"location":"isaac_sim/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Model Assessment:</p> <pre><code># Evaluate on validation set (Step 6 in local_train.ipynb)\nvalidation_results = model.evaluate(\n    validation_dataset,\n    metrics=['mAP', 'precision', 'recall']\n)\n\n# Visualize detection results\nvisualize_detections(\n    test_images,\n    predictions,\n    ground_truth\n)\n</code></pre>"},{"location":"isaac_sim/#best-practices","title":"Best Practices","text":"<p>Dataset Quality: - Diverse Scenarios: Multiple lighting conditions - Realistic Physics: Accurate object interactions - Balanced Classes: Equal representation of object types - Domain Transfer: Gradual real-to-synthetic adaptation</p>"},{"location":"isaac_sim/#software-in-the-loop-development-course-4","title":"\ud83d\udd27 Software-in-the-Loop Development (Course 4)","text":""},{"location":"isaac_sim/#overview-omnigraph-visual-programming","title":"Overview: OmniGraph Visual Programming","text":"<p>Software-in-the-Loop (SIL) development enables rapid prototyping and testing of robotics algorithms in simulation before hardware deployment. Isaac Sim's OmniGraph provides visual programming capabilities for complex robotics workflows.</p>"},{"location":"isaac_sim/#key-objectives","title":"Key Objectives","text":"<ul> <li>Action Graph Construction: Build visual programming workflows</li> <li>ROS 2 Integration: Seamless communication with ROS ecosystem</li> <li>Controller Development: Implement custom robot behaviors</li> <li>Real-time Testing: Validate algorithms in simulation</li> </ul>"},{"location":"isaac_sim/#ros-2-bridge-setup","title":"ROS 2 Bridge Setup","text":""},{"location":"isaac_sim/#prerequisites_1","title":"Prerequisites","text":"<p>Environment Configuration:</p> <pre><code># Source ROS 2 installation\nsource /opt/ros/humble/setup.bash\n\n# Verify ROS 2 environment\necho $ROS_DISTRO  # Should output: humble\n</code></pre>"},{"location":"isaac_sim/#isaac-sim-ros-2-integration","title":"Isaac Sim ROS 2 Integration","text":"<p>Enable ROS 2 Bridge: 1. Launch Isaac Sim: Ensure ROS 2 environment is sourced 2. Extension Manager: <code>Window \u2192 Extensions</code> 3. Search ROS: Find \"ROS2 Bridge\" extension 4. Enable Extension: Toggle ROS2 Bridge to \"ON\"</p> <p>Verification:</p> <pre><code># Check ROS 2 topics from Isaac Sim\nros2 topic list\n\n# Expected topics:\n# /clock\n# /tf\n# /tf_static\n</code></pre>"},{"location":"isaac_sim/#omnigraph-action-graphs","title":"OmniGraph Action Graphs","text":""},{"location":"isaac_sim/#graph-editor-access","title":"Graph Editor Access","text":"<p>Open Visual Scripting:</p> <pre><code>Window \u2192 Visual Scripting \u2192 Action Graph\n</code></pre>"},{"location":"isaac_sim/#basic-robot-control-graph","title":"Basic Robot Control Graph","text":"<p>Essential Nodes: - On Playback Tick: Execution trigger - ROS2 Subscribe Twist: Velocity commands - Articulation Controller: Joint control - ROS2 Publish Transform: Pose feedback</p> <p>Graph Construction:</p> <pre><code># Example OmniGraph setup via Python\nimport omni.graph.core as og\n\n# Create new graph\ngraph_path = \"/ActionGraph\"\nkeys = og.Controller.Keys\n(graph, nodes, _, _) = og.Controller.edit(\n    graph_path,\n    {\n        keys.CREATE_NODES: [\n            (\"OnPlaybackTick\", \"omni.graph.action.OnPlaybackTick\"),\n            (\"ROS2SubscribeTwist\", \"omni.isaac.ros2_bridge.ROS2SubscribeTwist\"),\n            (\"DifferentialController\", \"omni.isaac.wheeled_robots.DifferentialController\"),\n            (\"ArticulationController\", \"omni.isaac.core_nodes.IsaacArticulationController\")\n        ],\n        keys.CONNECT: [\n            (\"OnPlaybackTick.outputs:tick\", \"ROS2SubscribeTwist.inputs:execIn\"),\n            (\"ROS2SubscribeTwist.outputs:linearVelocity\", \"DifferentialController.inputs:linearVelocity\"),\n            (\"ROS2SubscribeTwist.outputs:angularVelocity\", \"DifferentialController.inputs:angularVelocity\")\n        ]\n    }\n)\n</code></pre>"},{"location":"isaac_sim/#advanced-control-workflows","title":"Advanced Control Workflows","text":""},{"location":"isaac_sim/#multi-robot-coordination","title":"Multi-Robot Coordination","text":"<p>Namespace Management:</p> <pre><code># Configure multiple robot namespaces\nrobot_configs = [\n    {\"namespace\": \"/robot_1\", \"topic\": \"/robot_1/cmd_vel\"},\n    {\"namespace\": \"/robot_2\", \"topic\": \"/robot_2/cmd_vel\"},\n    {\"namespace\": \"/robot_3\", \"topic\": \"/robot_3/cmd_vel\"}\n]\n\n# Create separate graphs for each robot\nfor config in robot_configs:\n    create_robot_control_graph(config)\n</code></pre>"},{"location":"isaac_sim/#sensor-integration-graph","title":"Sensor Integration Graph","text":"<p>Perception Pipeline:</p> <pre><code># Camera + LiDAR fusion graph\nsensor_nodes = {\n    \"camera_info\": \"omni.isaac.ros2_bridge.ROS2CameraHelper\",\n    \"lidar_scan\": \"omni.isaac.ros2_bridge.ROS2PublishLaserScan\",\n    \"point_cloud\": \"omni.isaac.ros2_bridge.ROS2PublishPointCloud\",\n    \"tf_publisher\": \"omni.isaac.ros2_bridge.ROS2PublishTransformTree\"\n}\n</code></pre>"},{"location":"isaac_sim/#real-time-robot-control","title":"Real-time Robot Control","text":""},{"location":"isaac_sim/#teleoperation-setup","title":"Teleoperation Setup","text":"<p>ROS 2 Teleop Integration:</p> <pre><code># Install teleop packages\nsudo apt install ros-humble-teleop-twist-keyboard\nsudo apt install ros-humble-joy ros-humble-teleop-twist-joy\n\n# Launch keyboard control\nros2 run teleop_twist_keyboard teleop_twist_keyboard \\\n    --ros-args --remap cmd_vel:=/isaac_sim/cmd_vel\n\n# Launch joystick control\nros2 launch teleop_twist_joy teleop-launch.py \\\n    joy_config:='xbox' \\\n    cmd_vel_topic:='/isaac_sim/cmd_vel'\n</code></pre>"},{"location":"isaac_sim/#navigation-stack-integration","title":"Navigation Stack Integration","text":"<p>Nav2 with Isaac Sim:</p> <pre><code># Launch Isaac Sim with navigation\nros2 launch isaac_ros_navigation isaac_sim_navigation.launch.py\n\n# Start Nav2 stack\nros2 launch nav2_bringup navigation_launch.py \\\n    use_sim_time:=true \\\n    map:=/path/to/map.yaml\n\n# Send navigation goals\nros2 topic pub /goal_pose geometry_msgs/PoseStamped \\\n    '{header: {frame_id: \"map\"}, \n      pose: {position: {x: 2.0, y: 1.0, z: 0.0}}}'\n</code></pre>"},{"location":"isaac_sim/#custom-node-development","title":"Custom Node Development","text":""},{"location":"isaac_sim/#python-extension-nodes","title":"Python Extension Nodes","text":"<p>Custom Behavior Implementation:</p> <pre><code>import omni.graph.core as og\nfrom omni.isaac.core_nodes import BaseResetNode\n\nclass CustomRobotBehavior(BaseResetNode):\n    @staticmethod\n    def compute(db) -&gt; bool:\n        # Custom robot logic\n        sensor_data = db.inputs.sensor_input\n\n        # Process sensor data\n        processed_data = process_sensors(sensor_data)\n\n        # Generate control commands\n        control_output = generate_commands(processed_data)\n\n        # Output to robot\n        db.outputs.control_command = control_output\n        return True\n</code></pre>"},{"location":"isaac_sim/#c-performance-nodes","title":"C++ Performance Nodes","text":"<p>High-Performance Computing:</p> <pre><code>#include &lt;omni/graph/core/Node.h&gt;\n#include &lt;omni/isaac/core/RigidBodyAPI.h&gt;\n\nclass HighFrequencyController : public omni::graph::core::Node {\npublic:\n    static bool compute(omni::graph::core::GraphContext&amp; context) {\n        // High-frequency control loop (1kHz+)\n        auto input_data = context.getInputData&lt;float&gt;(\"sensor_input\");\n\n        // Real-time processing\n        auto control_signal = computeControl(input_data);\n\n        // Output control commands\n        context.setOutputData(\"control_output\", control_signal);\n        return true;\n    }\n};\n</code></pre>"},{"location":"isaac_sim/#comprehensive-reference-links","title":"\ud83d\udcda Comprehensive Reference Links","text":""},{"location":"isaac_sim/#official-documentation","title":"Official Documentation","text":""},{"location":"isaac_sim/#nvidia-isaac-ecosystem","title":"NVIDIA Isaac Ecosystem","text":"<ul> <li>Isaac Sim Documentation: Complete Isaac Sim reference</li> <li>Isaac ROS Documentation: ROS 2 integration packages</li> <li>Isaac SDK: Robotics development framework</li> <li>Omniverse Platform: Collaboration and simulation platform</li> </ul>"},{"location":"isaac_sim/#core-technologies","title":"Core Technologies","text":"<ul> <li>USD Documentation: Universal Scene Description</li> <li>PhysX Documentation: Physics simulation</li> <li>RTX Rendering: Real-time ray tracing</li> <li>CUDA Programming: GPU acceleration</li> </ul>"},{"location":"isaac_sim/#isaac-ros-packages_1","title":"Isaac ROS Packages","text":""},{"location":"isaac_sim/#perception-ai","title":"Perception &amp; AI","text":"<ul> <li>isaac_ros_visual_slam: Visual-inertial SLAM</li> <li>isaac_ros_nvblox: 3D reconstruction</li> <li>isaac_ros_object_detection: AI object detection</li> <li>isaac_ros_apriltag: Fiducial markers</li> <li>isaac_ros_depth_segmentation: Depth-based segmentation</li> </ul>"},{"location":"isaac_sim/#navigation-control","title":"Navigation &amp; Control","text":"<ul> <li>isaac_ros_navigation: Navigation stack</li> <li>isaac_ros_map_localization: Localization algorithms</li> <li>isaac_ros_path_planning: Path planning</li> </ul>"},{"location":"isaac_sim/#machine-learning-training","title":"Machine Learning &amp; Training","text":""},{"location":"isaac_sim/#training-frameworks","title":"Training Frameworks","text":"<ul> <li>TAO Toolkit: Transfer learning toolkit</li> <li>NVIDIA Replicator: Synthetic data generation</li> <li>DeepStream SDK: Video analytics</li> <li>TensorRT: Inference optimization</li> </ul>"},{"location":"isaac_sim/#datasets-models","title":"Datasets &amp; Models","text":"<ul> <li>NGC Model Catalog: Pre-trained models</li> <li>LOCO Dataset: Logistics scene understanding</li> <li>Synthetic Data Workflows: Training pipelines</li> </ul>"},{"location":"isaac_sim/#hardware-integration","title":"Hardware Integration","text":""},{"location":"isaac_sim/#nvidia-platforms","title":"NVIDIA Platforms","text":"<ul> <li>Jetson Documentation: Edge AI computing</li> <li>Jetson AGX Orin: High-performance edge AI</li> <li>Nova Carter: Reference robot platform</li> </ul>"},{"location":"isaac_sim/#sensor-integration","title":"Sensor Integration","text":"<ul> <li>RealSense Integration: Intel depth cameras</li> <li>Velodyne LiDAR: 3D LiDAR sensors</li> <li>ZED Camera: Stereo vision</li> </ul>"},{"location":"isaac_sim/#community-learning","title":"Community &amp; Learning","text":""},{"location":"isaac_sim/#forums-support","title":"Forums &amp; Support","text":"<ul> <li>NVIDIA Developer Forums: Community support</li> <li>Isaac Sim GitHub: Code examples</li> <li>ROS 2 Documentation: ROS 2 ecosystem</li> </ul>"},{"location":"isaac_sim/#tutorials-examples","title":"Tutorials &amp; Examples","text":"<ul> <li>Isaac Sim Tutorials: Step-by-step guides</li> <li>Isaac ROS Tutorials: ROS integration examples</li> <li>Omniverse Learning: Training resources</li> </ul>"},{"location":"isaac_sim/#research-publications","title":"Research &amp; Publications","text":""},{"location":"isaac_sim/#academic-resources","title":"Academic Resources","text":"<ul> <li>Isaac Sim Research Papers: Latest research</li> <li>Simulation-to-Reality Transfer: Domain adaptation</li> <li>Synthetic Data for Robotics: Data generation techniques</li> </ul>"},{"location":"isaac_sim/#industry-applications","title":"Industry Applications","text":"<ul> <li>Autonomous Vehicles: NVIDIA DRIVE platform</li> <li>Industrial Robotics: Manufacturing applications</li> <li>Healthcare Robotics: Medical applications</li> </ul>"},{"location":"isaac_sim/#hardware-in-the-loop-development-course-5","title":"\ud83c\udfed Hardware-in-the-Loop Development (Course 5)","text":""},{"location":"isaac_sim/#overview-real-hardware-integration","title":"Overview: Real Hardware Integration","text":"<p>Hardware-in-the-Loop (HIL) development bridges the gap between simulation and real-world deployment. This approach enables seamless transition from Isaac Sim to physical robots while maintaining consistent behavior and performance.</p>"},{"location":"isaac_sim/#key-objectives_1","title":"Key Objectives","text":"<ul> <li>Sim-to-Real Transfer: Deploy algorithms from simulation to hardware</li> <li>Hardware Validation: Test real sensors and actuators</li> <li>Performance Optimization: Optimize for real-world constraints</li> <li>Production Deployment: Scale to manufacturing environments</li> </ul>"},{"location":"isaac_sim/#physical-robot-setup","title":"Physical Robot Setup","text":""},{"location":"isaac_sim/#supported-hardware-platforms","title":"Supported Hardware Platforms","text":"<p>NVIDIA Jetson Series: - Jetson AGX Orin: High-performance edge AI (275 TOPS) - Jetson Orin NX: Compact AI computing (100 TOPS) - Jetson Orin Nano: Entry-level edge AI (40 TOPS)</p> <p>Reference Platforms: - Nova Carter: Complete AMR development platform - Isaac AMR: Autonomous mobile robot reference - Custom Platforms: Integration with existing hardware</p>"},{"location":"isaac_sim/#hardware-configuration","title":"Hardware Configuration","text":"<p>Jetson Setup:</p> <pre><code># Flash JetPack 5.1.2+\nsudo sdkmanager --cli install \\\n    --logintype devzone \\\n    --product Jetson \\\n    --version 5.1.2 \\\n    --targetos Linux \\\n    --host \\\n    --target JETSON_AGX_ORIN_TARGETS \\\n    --flash all\n\n# Install Isaac ROS\nsudo apt update\nsudo apt install -y python3-rosdep\nrosdep init &amp;&amp; rosdep update\n\n# Clone Isaac ROS workspace\nmkdir -p ~/workspaces/isaac_ros-dev/src\ncd ~/workspaces/isaac_ros-dev/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\n</code></pre>"},{"location":"isaac_sim/#sensor-integration_1","title":"Sensor Integration","text":""},{"location":"isaac_sim/#camera-systems","title":"Camera Systems","text":"<p>Supported Cameras: - Intel RealSense D435i/D455: RGB-D + IMU - Stereolabs ZED 2i: Stereo vision + IMU - NVIDIA Hawk: Stereo cameras for Nova Carter - USB/CSI Cameras: Standard RGB cameras</p> <p>RealSense Integration:</p> <pre><code># Install RealSense SDK\nsudo apt install ros-humble-realsense2-camera\nsudo apt install ros-humble-realsense2-description\n\n# Launch RealSense with Isaac ROS\nros2 launch realsense2_camera rs_launch.py \\\n    enable_rgbd:=true \\\n    enable_sync:=true \\\n    align_depth.enable:=true \\\n    enable_color:=true \\\n    enable_depth:=true\n\n# Verify camera topics\nros2 topic list | grep camera\n# /camera/color/image_raw\n# /camera/depth/image_rect_raw\n# /camera/camera_info\n</code></pre>"},{"location":"isaac_sim/#lidar-integration_1","title":"LiDAR Integration","text":"<p>Supported LiDAR: - Velodyne VLP-16/VLP-32: 3D point clouds - Ouster OS1/OS2: High-resolution scanning - Livox Mid-360: Solid-state LiDAR - RPLiDAR A1/A3: 2D scanning</p> <p>Velodyne Setup:</p> <pre><code># Install Velodyne drivers\nsudo apt install ros-humble-velodyne\nsudo apt install ros-humble-velodyne-pointcloud\n\n# Configure network (static IP)\nsudo ip addr add 192.168.1.100/24 dev eth0\n\n# Launch Velodyne\nros2 launch velodyne velodyne-all-nodes-VLP16-launch.py\n\n# Verify point cloud\nros2 topic echo /velodyne_points --field data\n</code></pre>"},{"location":"isaac_sim/#real-time-control-systems","title":"Real-time Control Systems","text":""},{"location":"isaac_sim/#motor-controllers","title":"Motor Controllers","text":"<p>Supported Controllers: - CAN Bus: Industrial motor controllers - Ethernet/IP: Factory automation protocols - Serial/UART: Simple motor interfaces - GPIO: Direct hardware control</p> <p>CAN Bus Integration:</p> <pre><code># Python CAN interface\nimport can\nimport rclpy\nfrom geometry_msgs.msg import Twist\n\nclass CANMotorController:\n    def __init__(self):\n        # Initialize CAN bus\n        self.bus = can.interface.Bus(\n            channel='can0',\n            bustype='socketcan',\n            bitrate=500000\n        )\n\n        # ROS 2 subscriber\n        self.cmd_sub = self.create_subscription(\n            Twist, '/cmd_vel', self.cmd_callback, 10\n        )\n\n    def cmd_callback(self, msg):\n        # Convert ROS Twist to CAN messages\n        left_speed = msg.linear.x - msg.angular.z * 0.5\n        right_speed = msg.linear.x + msg.angular.z * 0.5\n\n        # Send CAN commands\n        self.send_motor_command(0x101, left_speed)\n        self.send_motor_command(0x102, right_speed)\n\n    def send_motor_command(self, can_id, speed):\n        # Create CAN message\n        data = struct.pack('&lt;f', speed)\n        message = can.Message(\n            arbitration_id=can_id,\n            data=data,\n            is_extended_id=False\n        )\n        self.bus.send(message)\n</code></pre>"},{"location":"isaac_sim/#navigation-deployment","title":"Navigation Deployment","text":""},{"location":"isaac_sim/#nav2-hardware-integration","title":"Nav2 Hardware Integration","text":"<p>Complete Navigation Stack:</p> <pre><code># Launch hardware drivers\nros2 launch robot_bringup sensors.launch.py\n\n# Start localization\nros2 launch nav2_bringup localization_launch.py \\\n    map:=/path/to/warehouse_map.yaml \\\n    use_sim_time:=false\n\n# Launch navigation\nros2 launch nav2_bringup navigation_launch.py \\\n    use_sim_time:=false \\\n    params_file:=/path/to/nav2_params.yaml\n\n# Start behavior trees\nros2 launch nav2_bringup bt_navigator_launch.py\n</code></pre> <p>Parameter Tuning:</p> <pre><code># nav2_params.yaml - Hardware-specific tuning\ncontroller_server:\n  ros__parameters:\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      global_frame: odom\n      robot_base_frame: base_link\n      rolling_window: true\n      width: 3\n      height: 3\n      resolution: 0.05\n</code></pre>"},{"location":"isaac_sim/#performance-optimization_1","title":"Performance Optimization","text":""},{"location":"isaac_sim/#real-time-constraints","title":"Real-time Constraints","text":"<p>System Configuration:</p> <pre><code># Set CPU governor to performance\nsudo cpufreq-set -g performance\n\n# Configure real-time priorities\nsudo sysctl -w kernel.sched_rt_runtime_us=950000\nsudo sysctl -w kernel.sched_rt_period_us=1000000\n\n# Set process priorities\nsudo chrt -f 80 ros2 run isaac_ros_visual_slam isaac_ros_visual_slam\nsudo chrt -f 70 ros2 run nav2_controller controller_server\n</code></pre> <p>Memory Optimization:</p> <pre><code># Optimize ROS 2 middleware\nexport RMW_IMPLEMENTATION=rmw_cyclonedx_cpp\nexport CYCLONEDX_URI='&lt;CycloneDX&gt;&lt;Domain&gt;&lt;General&gt;&lt;NetworkInterfaceAddress&gt;lo&lt;/NetworkInterfaceAddress&gt;&lt;/General&gt;&lt;/Domain&gt;&lt;/CycloneDX&gt;'\n\n# Configure DDS settings\nexport ROS_DOMAIN_ID=42\nexport RMW_FASTRTPS_USE_QOS_FROM_XML=1\n</code></pre>"},{"location":"isaac_sim/#production-deployment","title":"Production Deployment","text":""},{"location":"isaac_sim/#fleet-management","title":"Fleet Management","text":"<p>Multi-Robot Coordination:</p> <pre><code># Fleet management system\nclass FleetManager:\n    def __init__(self):\n        self.robots = {}\n        self.task_queue = []\n\n        # ROS 2 fleet interfaces\n        self.fleet_state_pub = self.create_publisher(\n            FleetState, '/fleet_state', 10\n        )\n\n    def assign_task(self, robot_id, task):\n        # Task assignment logic\n        if robot_id in self.robots:\n            self.robots[robot_id].assign_task(task)\n\n    def monitor_fleet(self):\n        # Health monitoring\n        for robot_id, robot in self.robots.items():\n            status = robot.get_status()\n            if status.battery_level &lt; 0.2:\n                self.schedule_charging(robot_id)\n</code></pre>"},{"location":"isaac_sim/#monitoring-diagnostics","title":"Monitoring &amp; Diagnostics","text":"<p>System Health Monitoring:</p> <pre><code># Launch diagnostics\nros2 launch robot_diagnostics diagnostics.launch.py\n\n# Monitor system resources\nros2 run robot_diagnostics system_monitor\n\n# Check sensor health\nros2 run robot_diagnostics sensor_diagnostics\n\n# View diagnostic dashboard\nros2 run rqt_robot_monitor rqt_robot_monitor\n</code></pre>"},{"location":"isaac_sim/#troubleshooting-maintenance","title":"Troubleshooting &amp; Maintenance","text":""},{"location":"isaac_sim/#common-issues","title":"Common Issues","text":"<p>Hardware Connectivity:</p> <pre><code># Check USB devices\nlsusb | grep -E \"Intel|RealSense|Velodyne\"\n\n# Verify network interfaces\nip addr show\nping 192.168.1.201  # LiDAR IP\n\n# Test CAN bus\ncandump can0\ncanecho can0\n</code></pre> <p>Performance Debugging:</p> <pre><code># Monitor CPU usage\nhtop\n\n# Check memory usage\nfree -h\n\n# Monitor ROS 2 performance\nros2 run ros2_performance performance_test\n\n# Profile specific nodes\nros2 run ros2_profiling profile_node /isaac_ros_visual_slam\n</code></pre>"},{"location":"isaac_sim/#maintenance-procedures","title":"Maintenance Procedures","text":"<p>Regular Maintenance:</p> <pre><code># Update system packages\nsudo apt update &amp;&amp; sudo apt upgrade\n\n# Update Isaac ROS\ncd ~/workspaces/isaac_ros-dev\ngit pull\ncolcon build --symlink-install\n\n# Clean build artifacts\nrm -rf build/ install/ log/\ncolcon build --symlink-install\n\n# Backup configuration\ntar -czf robot_config_$(date +%Y%m%d).tar.gz \\\n    /opt/ros/humble/share/robot_config/\n</code></pre>"},{"location":"isaac_sim/#conclusion","title":"\ud83c\udfaf Conclusion","text":"<p>This comprehensive guide covers the complete Isaac Sim ecosystem, from basic setup to advanced hardware-in-the-loop deployment. The integration of Isaac ROS provides a robust foundation for developing, testing, and deploying robotics applications across simulation and real-world environments.</p>"},{"location":"isaac_sim/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Unified Development: Seamless sim-to-real workflow</li> <li>Scalable Architecture: From prototyping to production</li> <li>Rich Ecosystem: Comprehensive tooling and community support</li> <li>Performance Optimization: Real-time capabilities for demanding applications</li> </ul> <p>For the latest updates and community contributions, visit the Isaac ROS GitHub Organization and join the NVIDIA Developer Community.</p>"},{"location":"isaac_sim_embodied_ai/","title":"Digital Twin in CMPE249: Intelligent Autonomous Systems","text":""},{"location":"isaac_sim_embodied_ai/#module-4-digital-twinning-imitation-learning-and-sim-to-real-transfer","title":"Module 4: Digital Twinning, Imitation Learning, and Sim-to-Real Transfer","text":"<ul> <li>Instructor: Kaikai Liu</li> <li>Duration: 4 Weeks (8 Lectures + 1 Capstone Project)</li> <li>Prerequisites: Python, Introduction to Deep Learning, Basic ROS2 concepts</li> </ul>"},{"location":"isaac_sim_embodied_ai/#module-overview","title":"Module Overview","text":"<p>This advanced module integrates NVIDIA Omniverse Isaac Sim with modern imitation learning using Hugging Face LeRobot. You will bridge simulation and reality by constructing a Digital Twin workflow. The focus is an end-to-end pipeline:</p> <ul> <li>Author photorealistic, physics-accurate environments and robots in Isaac Sim (USD-first)</li> <li>Teleoperate in simulation to collect expert demonstrations with physical controllers</li> <li>Curate multi-modal datasets suitable for imitation learning and foundation models</li> <li>Fine-tune policies (e.g., ACT, Diffusion) on task-specific data</li> <li>Deploy models back into simulation for evaluation and iterate toward sim-to-real transfer</li> </ul> <p>References: see OpenUSD Foundations, OpenUSD Applied, and Omniverse Kit for USD scene authoring and Kit workflows.</p> <p>Example Student Demo Previews (final project exemplars):</p> <p></p> <p></p> <p>Each preview image links to the full Google Drive video. These demos illustrate expected outcomes: end-to-end teleop data collection, dataset curation, policy training, and deployment in the Digital Twin.</p>"},{"location":"isaac_sim_embodied_ai/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, students will be able to:</p> <ul> <li>Architect a Digital Twin: build physics-accurate robotic environments with calibrated sensors (LiDAR, RGB-D)</li> <li>Implement teleoperation pipelines: map PS5 DualSense inputs and real-arm joint states to virtual robots</li> <li>Engineer datasets: record synchronized observations and actions; clean, label, and structure for LeRobot</li> <li>Train foundation policies: fine-tune ACT and Diffusion-based models on task-specific demonstrations</li> <li>Evaluate in simulation: measure success rate, time-to-completion, and collisions under domain randomization</li> <li>Plan sim-to-real transfer: identify gaps, design adaptation strategies, and safety-check deployments</li> </ul>"},{"location":"isaac_sim_embodied_ai/#lecture-outline-syllabus","title":"Lecture Outline &amp; Syllabus","text":""},{"location":"isaac_sim_embodied_ai/#week-1-the-environment-the-body-isaac-sim-basics","title":"Week 1: The Environment &amp; The Body (Isaac Sim Basics)","text":""},{"location":"isaac_sim_embodied_ai/#lecture-41-introduction-to-omniverse-usd","title":"Lecture 4.1: Introduction to Omniverse &amp; USD","text":"<p>Understanding the Universal Scene Description (USD) format.</p> <p>Importing URDFs (Universal Robot Description Format) into Isaac Sim.</p> <p>Rigging the robot: Articulation Roots, Joints, and Drives.</p> <p>Lab Activity: Importing a Mobile Manipulator (e.g., Franka Emika or custom wheeled robot) into a warehouse scene.</p> <p>Lab Deliverables: - USD stage with articulated robot and base environment - Attached sensors (RGB-D, LiDAR) validated via Kit viewport and simple OmniGraph - Screenshot or short clip demonstrating sensor outputs</p>"},{"location":"isaac_sim_embodied_ai/#lecture-42-sensorization-graph-control","title":"Lecture 4.2: Sensorization &amp; Graph Control","text":"<p>Attaching Sensors: RTX Lidar, RGB-D Cameras, and IMUs.</p> <p>OmniGraph Basics: Visual scripting for sensor publishing.</p> <p>Ground Truth vs. Noisy Data: Configuring sensor noise models for realism.</p> <p>Lab Checklist: - Create an OmniGraph pipeline for publishing camera and LiDAR frames - Enable noise models and compare outputs against ground truth labels - Document sensor calibration parameters (FOV, resolution, extrinsics)</p> <p>Technical Details: - Camera intrinsics: choose focal length to match target FOV; verify projection against known checkerboard/Charuco pattern in-sim - Extrinsics: fix sensor mounts on the robot link with USD Xform; record transform tree for dataset metadata - RTX LiDAR: configure horizontal/vertical resolution, max range, and material reflectance; validate point cloud density and noise - OmniGraph: use Sensor \u2192 Writer nodes to publish frames; ensure consistent timestamping via simulation time</p>"},{"location":"isaac_sim_embodied_ai/#week-2-the-digital-twin-interface-teleoperation","title":"Week 2: The Digital Twin Interface (Teleoperation)","text":""},{"location":"isaac_sim_embodied_ai/#lecture-43-controller-mapping-the-human-in-the-loop","title":"Lecture 4.3: Controller Mapping (The Human-in-the-Loop)","text":"<p>Interfacing hardware with Python (evdev, inputs, or ROS2 Joy).</p> <p>Mapping Strategy:</p> <p>Locomotion: Mapping PS5 analog sticks to differential/holonomic drive commands.</p> <p>Manipulation: Introduction to Inverse Kinematics (IK) solvers in Isaac (Lula/RMPflow).</p>"},{"location":"isaac_sim_embodied_ai/#lecture-44-physical-to-digital-bridge-the-twin-aspect","title":"Lecture 4.4: Physical-to-Digital Bridge (The \"Twin\" Aspect)","text":"<p>Concept: The Physical \"Leader\" Arm and the Virtual \"Follower.\"</p> <p>Streaming joint states from a physical arm (e.g., widowX, xArm) into Isaac Sim via TCP/IP or ROS2.</p> <p>Synchronization challenges: Latency, frequency matching, and safety limits.</p> <p>Practical Notes: - Use <code>pygame</code> for PS5 input capture; consider ROS2 <code>joy</code> for ROS-native workflows - Log controller events alongside simulation timestamps for dataset alignment - Implement basic safety guards (rate limiting, joint bounds) during teleop</p> <p>Teleop Mapping Details: - DualSense axes (typical): left stick (x: axis 0, y: axis 1), right stick (x: axis 3, y: axis 4); triggers as analog buttons (L2/R2) - Base control: map left stick to linear/angular velocities; apply deadzone and smoothing filters - Arm control: use right stick for EE position deltas; combine with shoulder buttons for mode switching (position vs. orientation) - IK &amp; control: prefer Lula/RMPflow for smooth joint targets; clamp velocities and acceleration; enforce joint limit safety</p>"},{"location":"isaac_sim_embodied_ai/#week-3-imitation-learning-with-hugging-face-lerobot","title":"Week 3: Imitation Learning with Hugging Face LeRobot","text":""},{"location":"isaac_sim_embodied_ai/#lecture-45-data-collection-for-behavioral-cloning","title":"Lecture 4.5: Data Collection for Behavioral Cloning","text":"<p>Defining the Task: Pick-and-Place or Navigation.</p> <p>The Dataset Format: Recording observation.images, observation.state, and action (joint velocities/positions) at 30Hz-50Hz.</p> <p>Using the LeRobot dataset structure (Hugging Face Hub standards).</p> <p>Dataset Schema (recommended): - <code>observation.images</code>: resized RGB (e.g., 320x240), optionally depth - <code>observation.state</code>: joint positions/velocities, end-effector pose - <code>action</code>: joint targets or velocity commands at 30\u201350 Hz - <code>meta</code>: timestamps, episode ids, task labels</p> <p>Dataset Engineering Details: - File structure: episodes segmented (e.g., <code>episode_0001/frames</code>, <code>episode_0001/actions.npy</code>, <code>meta.json</code>) - Time sync: use simulation clock; align image timestamps with action timestamps; store frequency and unit in metadata - Compression: PNG or JPEG for images; consider chunked arrays (<code>.npz</code>/Arrow) for states/actions - Quality control: remove outlier frames; ensure consistent action scaling; document normalization</p>"},{"location":"isaac_sim_embodied_ai/#lecture-46-model-architectures-fine-tuning","title":"Lecture 4.6: Model Architectures &amp; Fine-Tuning","text":"<p>Introduction to LeRobot: Library structure and pre-trained models.</p> <p>Theory: ACT (Action Chunking with Transformers) and Diffusion Policies.</p> <p>Fine-tuning: Taking a foundation model trained on large datasets (e.g., Open X-Embodiment) and adapting it to our specific Isaac Sim task.</p> <p>Training Tips: - Start with ACT for temporally coherent actions; compare with Diffusion for robustness - Normalize observations (images/state) consistently across train and eval - Use modest horizons first (8\u201316) and increase as stability improves - Monitor success metrics, not just loss; add curriculum by randomizing object positions - Hardware: ensure GPU memory headroom; enable mixed precision where supported</p>"},{"location":"isaac_sim_embodied_ai/#week-4-deployment-evaluation","title":"Week 4: Deployment &amp; Evaluation","text":""},{"location":"isaac_sim_embodied_ai/#lecture-47-inference-evaluation-in-the-loop","title":"Lecture 4.7: Inference &amp; Evaluation in the Loop","text":"<p>Loading the trained PyTorch model.</p> <p>Closing the loop: Feeding live Isaac Sim camera frames to the model $\\rightarrow$ Predicting actions $\\rightarrow$ Applying actions to the Sim robot.</p> <p>Designing Evaluation Metrics: Success rate, time-to-completion, collision checks.</p> <p>Evaluation Extensions: - Domain randomization sweeps (textures, lighting, dynamics) to test robustness - Ablation studies comparing ACT vs. Diffusion policies and sensor subsets - Logging and visualization of trajectories for qualitative analysis</p> <p>Evaluation Protocol: - Define task success criteria (e.g., object grasped within N seconds, minimal collisions) - Run multiple seeds and initializations; report mean \u00b1 std for metrics - Record per-episode traces (images, actions, states) for post-hoc analysis - Compare policies under identical randomization settings to isolate effects</p>"},{"location":"isaac_sim_embodied_ai/#lecture-48-the-path-to-sim-to-real-advanced","title":"Lecture 4.8: The Path to Sim-to-Real (Advanced)","text":"<p>Domain Randomization: Varying textures, lighting, and physics properties in Isaac to prevent overfitting.</p> <p>Strategy for transferring the LeRobot policy to the physical hardware.</p> <p>Sim-to-Real Checklist: - Calibrate real sensors to match simulated intrinsics/extrinsics - Apply domain adaptation (style augmentation, feature normalization) to bridge sim-to-real - Enforce safety: rate limiting, emergency stop, workspace constraints - Incremental deployment: dry-run without actuation, then low-power tests, then full autonomy</p> <p>Assessment &amp; Grading: - Labs (Weeks 1\u20133): 30% - Capstone project (pipeline completeness, reproducibility): 45% - Final evaluation report (metrics, analysis, video demo): 20% - Participation (discussions, code reviews): 5%</p>"},{"location":"isaac_sim_embodied_ai/#major-course-assignment-the-digital-puppeteer","title":"Major Course Assignment: \"The Digital Puppeteer\"","text":"<p>Objective: Create an autonomous clean-up robot pipeline. Hardware: PC with NVIDIA RTX GPU, PS5 Controller, (Optional) Desktop Robotic Arm.</p> <p>Phase 1: Scene &amp; Robot Setup (Isaac Sim)</p> <p>Create a \"Tabletop\" environment in Isaac Sim with 3 random objects (cubes/cans).</p> <p>Import a mobile manipulator robot.</p> <p>Deliverable: A USD file where the robot can be controlled via keyboard, and sensors (Camera/Lidar) are visualizing data.</p> <p>Phase 2: The Teleop &amp; Data Collector</p> <p>Write a Python script teleop_collect.py.</p> <p>Locomotion: Bind PS5 Left Analog Stick to robot base velocity.</p> <p>Manipulation:</p> <p>Option A (Sim-Only): Bind PS5 Right Stick to End-Effector IK target.</p> <p>Option B (Digital Twin): Connect physical arm USB. Read joint angles. Apply these angles directly to the Isaac Sim robot joints (ignoring physics/collision for the \"ghost\" arm, or using PD control for the physics arm).</p> <p>Recording: Implement a \"Record\" button (PS5 'X' button). When held, save synchronized frames (320x240 resized) and joint positions to a local folder structured for LeRobot.</p> <p>Deliverable: A dataset of 50 successful \"pick up object\" demonstrations.</p> <p>Milestones &amp; Rubric: - M1 (Environment &amp; Sensors): robot imported, sensors streaming (10%) - M2 (Teleop &amp; Recorder): controller mapping, synchronized logging (15%) - M3 (Dataset &amp; Training): formatted dataset, training run completes (20%) - M4 (Inference &amp; Evaluation): autonomous run with metrics + video (25%) - Report: method, results, lessons learned, next steps (30%)</p> <p>Phase 3: Training with LeRobot</p> <p>Convert your raw dataset to the LeRobot / Hugging Face dataset format (.arrow or standard folders).</p> <p>Use the provided Colab/Local notebook to fine-tune a Diffusion Policy.</p> <p>Training configurations:</p> <p>Batch size: 32</p> <p>Epochs: 500</p> <p>Horizon: 16 steps (Action Chunking).</p> <p>Deliverable: Training loss graphs and the saved model weights (policy.pt).</p> <p>Phase 4: Deployment (Inference)</p> <p>Write eval_policy.py.</p> <p>Load the trained policy.</p> <p>Reset the Isaac Sim environment.</p> <p>Run the robot autonomously. The script should capture camera data from Sim, pass it to the model, and execute the returned actions.</p> <p>Deliverable: A video recording of the robot successfully performing the task autonomously in Isaac Sim.</p> <p>Submission Guidelines: - Repository: include scripts (<code>teleop_collect.py</code>, <code>eval_policy.py</code>), configs, and README - Data: sample episode (images + actions) and dataset conversion script/notebook - Report: PDF with methodology, metrics tables, and links to demo video</p>"},{"location":"isaac_sim_embodied_ai/#technical-stack-implementation-details","title":"Technical Stack &amp; Implementation Details","text":""},{"location":"isaac_sim_embodied_ai/#software-requirements","title":"Software Requirements","text":"<ul> <li>OS: Ubuntu 20.04/22.04</li> <li>Simulation: NVIDIA Isaac Sim 4.0+</li> <li>ML Framework: PyTorch, Hugging Face LeRobot, Diffusers</li> <li>Control: pygame (for PS5), rospy/rclpy (optional, for ROS bridge)</li> </ul>"},{"location":"isaac_sim_embodied_ai/#setup-checklist","title":"Setup Checklist","text":"<ul> <li>Isaac Sim 4.0+ installed and validated (see <code>docs/isaac_sim.md</code>)</li> <li>Python environment with PyTorch, LeRobot, Diffusers, and required drivers</li> <li>PS5 controller paired and recognized (<code>pygame</code> or <code>ros2 joy</code>)</li> <li>Optional ROS2 bridge configured for joint streaming</li> </ul> <p>Domain Randomization &amp; Synthetic Data: - Use Omniverse Replicator to randomize materials, lighting, and physics parameters - Vary textures, backgrounds, object positions, and physical properties across episodes - Export synthetic datasets with perfect ground truth for benchmarking and pretraining</p>"},{"location":"isaac_sim_embodied_ai/#resources","title":"Resources","text":"<ul> <li>USD &amp; Kit: OpenUSD Foundations, OpenUSD Applied, Omniverse Kit</li> <li>Isaac Sim: Isaac Sim</li> <li>ROS 2 Integration: ROS 2 Integration</li> </ul>"},{"location":"isaac_sim_embodied_ai/#important-reference-links","title":"Important Reference Links","text":"<ul> <li>NVIDIA Isaac Sim Docs: https://docs.omniverse.nvidia.com/isaacsim/latest/</li> <li>Omniverse Replicator (synthetic data): https://developer.nvidia.com/omniverse/replicator</li> <li>OmniGraph Overview: https://docs.omniverse.nvidia.com/kit/docs/omni.graph/latest/overview.html</li> <li>USD (Pixar): https://graphics.pixar.com/usd/docs/index.html</li> <li>Hugging Face LeRobot: https://github.com/huggingface/lerobot</li> <li>Open X-Embodiment (foundation dataset): https://arxiv.org/abs/2306.08764</li> <li>ACT (Action Chunking with Transformers): https://arxiv.org/abs/2304.13705</li> <li>Diffusion Policy for Robot Control: https://arxiv.org/abs/2303.01469</li> <li>ROS 2 Docs: https://docs.ros.org/en/</li> <li>pygame DualSense input: https://www.pygame.org/docs/</li> <li>RMPflow (Isaac): https://docs.omniverse.nvidia.com/isaacsim/latest/robotics_isaac/motion_generation.html</li> <li>Lula IK: https://docs.omniverse.nvidia.com/isaacsim/latest/robotics_isaac/ik_solver.html</li> </ul>"},{"location":"kit_app_streaming/","title":"Kit App Streaming","text":"<p>Course: Building and Deploying Digital Twin Applications With Omniverse Kit App Streaming</p>"},{"location":"kit_app_streaming/#setup-browser-based-streaming","title":"Setup browser-based streaming.","text":"<p>Kit app streaming means pixels streaming a Kit app to a client and allowing for the two apps to communicate. This is a peer-to-peer solution: one Kit app streamed to one client. </p> <p>For iterating and simpler proof-of-concept work, the deployment can be all local\u2014running on a single workstation. For scalable solutions, we add an API that can launch a requested application on demand. Current APIs are Omniverse Kit App Streaming API (OKAS), Omniverse Cloud (OVC), and Graphics Delivery Network (GDN). OVC and GDN are managed by NVIDIA where GDN is great for Product Configurators, and OVC for demanding and massively scaled applications like Digital Twins.</p> <p></p> <p>For this course we narrow this down to web and Python development. The infrastructure and Omniverse Kit App Streaming API deployment has already been set up for us. Unlike a production environment that supports a disaggregated scalable solution, for this course we run everything on a single remote node. However, the general development and deployment steps are not different from a production environment.</p> <p>The course is divided into three parts: first we create the Kit app and the web front end, then we deploy and test the solution, and finally we take a closer look at how to create custom communication between the apps.</p> <p></p> <p>Create USD Viewer From Template</p> <pre><code>cd kit-app-template\n./repo.sh template new\n#Use arrow keys to select: USD Viewer\n#Build the app by running:\n./repo.sh build\n</code></pre> <p>Prepare Front End Client: <code>web-viewer-sample</code></p> <pre><code>cd web-viewer-sample\nnpm install #pulls dependencies and NVIDIA Omniverse WebRTC Streaming Library\n</code></pre> <p>We need to define the IP address that the client will use to connect and stream the Kit application. Open <code>web-viewer-sample/stream.config.json</code>. Note the IP address in the server field. This is the IP address the client will use to connect to the Kit app. For example</p> <pre><code>{\n    \"$comment\": \"source can be ` , ` local`  or ` stream` \",\n    \"source\": \"local\",\n    \"stream\":\n    {\n        \"$comment\": \"Optional props if source is set to ` stream` .\",\n        \"appServer\": \"\",\n        \"streamServer\": \"\"\n    },\n    \"gfn\": {\n        \"$comment\": \"Required props if source is set to ` gfn` .\",\n        \"catalogClientId\": \"\",\n        \"clientId\": \"\",\n        \"cmsId\": 0\n    },\n    \"local\": {\n        \"$comment\": \"Required props if source is set to ` local` .\",\n        \"server\": \"20.163.55.176\"\n    }\n}\n\n</code></pre>"},{"location":"kit_app_streaming/#test-run","title":"Test Run","text":"<p>Launch the Kit app in headless mode. Navigate to VS Code and the terminal for the kit-app-template project.</p> <pre><code>./repo.sh launch -- --no-window\n#Select: my_company.my_usd_viewer_streaming.kit\n</code></pre> <p>Launch front end client, Navigate to VS Code and the terminal for the web-viewer-sample project.</p> <pre><code>npm run dev\n</code></pre> <p>Open the PORTS tab next to the TERMINAL tab in VS Code. hover over the URL for port 5173. Click the globe icon for Open in Browser. This should open a new browser tab. Edit the URL by removing /proxy/8080 such that the URL ends with :5173. For example, http://x-ov-90-560-6.development.dli-infra.nvidia.com:5173/.</p>"},{"location":"kit_app_streaming/#deploy","title":"Deploy","text":"<p>Containerize and push the app to a registry so the Omniverse Kit App Streaming API can make use of it</p>"},{"location":"omniverse_digitaltwins/","title":"Omniverse Digital Twins","text":""},{"location":"omniverse_digitaltwins/#course1-extend-omniverse-kit-applications-for-building-digital-twins","title":"Course1: Extend Omniverse Kit Applications for Building Digital Twins","text":"<p>link</p> <p>Download these Factory Assets to your local drive, and extract the contents of the zip folder.</p> <p>Use the custom application created from the USD Explorer template. Launch your custom application. At the opening screen, click New. This will open a new file in Layout mode.</p> <p>Your application includes two modes, Review and Layout, which correspond to links at the top center of the screen. Review is for end users of your application who need to navigate, review and add comments to the digital twin. Layout is optimized for power users of your application who assemble large scenes and execute changes.</p> <p>In Layout mode, open the file Factory_Lite.usd that you downloaded. Save the file as a .usda file. </p> <p></p> <p>Open the .usda file in VS Code or a text editor, and inspect its structure. In the .usda file, search for the term \u201c.usd\u201d.</p> <p>Rather than explicitly defining each mesh in the file, the Factory_Lite file references multiple USD files that contain individual items. These other USD files are located in the SubUSDs folder you downloaded as part of the Factory_Lite.zip file. </p> <p>In your application, navigate to the SubUSDs folder, and locate Vehicle_Hanger_Adjust.usd. Double-click it to open the asset in the viewport.</p> <p>Save the asset as a .usda file. Open the .usda file in VS Code, and inspect the file. The file contains a long list of vertex positions that define the asset\u2019s shape in 3D space.</p> <p>By having the Factory_Lite file reference individual USDs like Vehicle_Hanger_Adjust.usd, the Factory_Lite file itself is very lightweight and easy to edit. By keeping each asset in a separate USD file, it\u2019s easier to update an individual asset without having to deal with the entire scene.</p>"},{"location":"omniverse_digitaltwins/#course2-building-and-deploying-digital-twin-applications-with-omniverse-kit-app-streaming","title":"Course2: Building and Deploying Digital Twin Applications With Omniverse Kit App Streaming","text":"<p>link Contains GPU hours</p> <p>By leveraging NVIDIA Omniverse Kit App Streaming, you can stream these interactive 3D environments to web browsers and other front-end clients, facilitating remote real-time data visualization and remote collaboration and decision-making. We\u2019ll be demonstrating how to create and deploy web-based applications using Omniverse\u2019s Kit App Streaming sample.</p> <p>By the end of this lab, you\u2019ll be able to develop your own applications to enable real-time interaction and visualization, laying the groundwork for integrating IoT data and leveraging these technologies for actionable insights.</p>"},{"location":"omniverse_digitaltwins/#module-1-create","title":"Module 1: Create","text":""},{"location":"omniverse_digitaltwins/#kit-app","title":"Kit App","text":"<pre><code>./repo.sh template new\n</code></pre> <p>Use arrow keys to select: USD Viewer Build the app by running:</p> <pre><code>./repo.sh build\n./repo.sh launch -- --no-window\n</code></pre> <p>Select: my_company.my_usd_viewer_streaming.kit</p>"},{"location":"omniverse_digitaltwins/#web-front-end","title":"Web Front end","text":"<p>Prepare Front End Client: web-viewer-sample  - Sample code for requesting Kit app streaming sessions from OV Kit App Streaming API  - A sample front end client for embedding a streamed Kit app.</p> <p>In a new terminal</p> <pre><code>cd web-viewer-sample\nnpm install\n</code></pre> <p>This pulls dependencies into the project. The most critical of them is the NVIDIA Omniverse WebRTC Streaming Library. This is the library that can present a streamed Kit application and provides bi-directional messaging. When you develop your own front end client this is the recommended library to use.</p> <p>We need to define the IP address that the client will use to connect and stream the Kit application. This address should be the IP address for where the Kit application is running.Open web-viewer-sample/stream.config.json. Note the IP address in the server field. This is the IP address the client will use to connect to the Kit app.</p> <p>Launch front end client:</p> <pre><code>npm run dev\n</code></pre> <p>Open the PORTS tab next to the TERMINAL tab in VS Code. With the mouse, hover over the URL for port 5173. Click the globe icon for Open in Browser. This should open a new browser tab.</p> <p>Edit the URL by removing /proxy/8080 such that the URL ends with :5173. For example, http://x-ov-90-560-6.development.dli-infra.nvidia.com:5173/. You should now see the front end client in the new browser tab.</p> <p>Select the UI option for using with USD Viewer. Click Next. The front end client is now attempting to connect to a Kit app. You should now see the Kit app being streamed in the front end client. It may show progress indication for connecting to the Kit app and for loading a USD stage.</p>"},{"location":"omniverse_digitaltwins/#module-2-deploy","title":"Module 2: Deploy","text":""},{"location":"omniverse_kit/","title":"Omniverse Kit","text":"<p>Build custom applications, extensions, and tools with NVIDIA Omniverse Kit - the powerful development platform for creating USD-based applications.</p>"},{"location":"omniverse_kit/#course-an-introduction-to-developing-with-nvidia-omniverse","title":"Course: An Introduction to Developing With NVIDIA Omniverse","text":""},{"location":"omniverse_kit/#getting-started-with-kit-app-templates","title":"Getting Started with Kit App Templates","text":"<p>Prerequisites: - Access to the Kit App Template repository: https://github.com/NVIDIA-Omniverse/kit-app-template - Development environment set up with appropriate tooling</p>"},{"location":"omniverse_kit/#creating-your-first-application","title":"Creating Your First Application","text":"<p>Step 1: Initialize a New Template</p> <p>Open your terminal and run the template creation command for your operating system:</p> <p>Windows:</p> <pre><code>.\\repo.bat template new\n</code></pre> <p>Linux/macOS:</p> <pre><code>./repo.sh template new\n</code></pre> <p>This command creates a new application from Kit SDK resources using an interactive setup process.</p> <p>Step 2: Configure Your Application</p> <p>You'll be prompted with a series of configuration questions. Use the arrow keys to navigate options and press Enter to confirm each selection:</p> <pre><code>Do you accept the EULA (End User License Agreement)?\n(Select Yes or No with arrow keys): Yes\n[ctrl+c to Exit]\n? Select with arrow keys what you want to create: Application&gt;\n? Select with arrow keys your desired template: [kit_base_editor]: Kit Base Editor\n? Enter name of application .kit file [name-spaced, lowercase, alphanumeric]:\n    my_company.my_editor\n? Enter application_display_name: My Editor\n? Enter version: 0.1.0\n</code></pre> <p>Important: Choose Kit Base Editor as your template for this tutorial.</p>"},{"location":"omniverse_kit/#understanding-the-kit-file","title":"Understanding the .kit File","text":"<p>Location and Purpose: The generated <code>.kit</code> file can be found in <code>source\\apps\\</code> (or <code>source/apps/</code> on Linux/macOS). If you used the default name, it will be <code>my_company.my_editor.kit</code>.</p> <p>What is a .kit File? The <code>.kit</code> file serves as a manifest that defines: - Which extensions to load from the Omniverse ecosystem - Application configuration and dependencies - Building blocks that compose your custom application</p> <p>Each entry in the <code>.kit</code> file represents a modular component that contributes specific functionality to your application.</p>"},{"location":"omniverse_kit/#building-and-launching-your-application","title":"Building and Launching Your Application","text":"<p>Step 1: Build the Application</p> <p>Before running your application, you must compile it and prepare its extensions:</p> <p>Windows:</p> <pre><code>.\\repo.bat build\n</code></pre> <p>Linux/macOS:</p> <pre><code>./repo.sh build\n</code></pre> <p>The build process compiles your application and its extensions, preparing them for launch.</p> <p>Step 2: Launch the Application</p> <p>Start your newly created application:</p> <p>Windows:</p> <pre><code>.\\repo.bat launch\n</code></pre> <p>Linux/macOS:</p> <pre><code>./repo.sh launch\n</code></pre> <p>When prompted in the terminal, press Enter to select your application and begin the launch process.</p> <p>Note: The first launch may take several minutes as resources are initialized. Subsequent launches will be faster.</p>"},{"location":"omniverse_kit/#exploring-your-application","title":"Exploring Your Application","text":"<p>Default Interface: Your Kit Base Editor application includes four main menus: - File - File operations and project management - Edit - Editing tools and preferences - Create - Content creation tools - Window - Interface and panel management</p> <p>Menu Extensions in .kit File: These menus are defined by extensions in your <code>.kit</code> file:</p> <pre><code>\"omni.kit.menu.create\" = {} # Create menu\n\"omni.kit.menu.edit\" = {}  # Edit menu\n\"omni.kit.menu.file\" = {}  # File menu\n</code></pre>"},{"location":"omniverse_kit/#viewport-navigation","title":"Viewport Navigation","text":"<p>Interactive Controls: - Camera Movement: Right-click and hold (RMB) + WASD keys - Camera Speed: Middle scroll wheel while holding RMB - Camera Rotation: Drag with RMB - Focus Object: Press F key to center view on selected object - Combined Navigation: Use RMB + drag + WASD simultaneously for fluid movement</p>"},{"location":"omniverse_kit/#creating-a-usd-explorer-application","title":"Creating a USD Explorer Application","text":"<p>Now let's create a more advanced application using the USD Explorer template, which provides enhanced tools for reviewing and working with large USD content.</p>"},{"location":"omniverse_kit/#usd-explorer-setup","title":"USD Explorer Setup","text":"<p>Step 1: Create USD Explorer Template</p> <p>Run the template command again:</p> <p>Windows:</p> <pre><code>.\\repo.bat template new\n</code></pre> <p>Linux/macOS:</p> <pre><code>./repo.sh template new\n</code></pre> <p>This time, select USD Explorer as your template. You'll see additional questions about \"setup extension\" configuration.</p> <p>Step 2: Compare Applications</p> <p>In VS Code: 1. Open both <code>.kit</code> files: <code>my_company.my_editor.kit</code> and <code>my_company.my_usd_explorer.kit</code> 2. Arrange them side by side for comparison 3. Examine the <code>[dependencies]</code> sections - note the different extensions each application includes</p> <p>Step 3: Build and Launch USD Explorer</p> <p>Build the USD Explorer application:</p> <p>Windows:</p> <pre><code>.\\repo.bat build\n</code></pre> <p>Linux/macOS:</p> <pre><code>./repo.sh build\n</code></pre> <p>Then launch it:</p> <p>Windows:</p> <pre><code>.\\repo.bat launch\n</code></pre> <p>Linux/macOS:</p> <pre><code>./repo.sh launch\n</code></pre> <p>When prompted, select your new USD Explorer application.</p>"},{"location":"omniverse_kit/#extensions-building-blocks-of-kit-applications","title":"Extensions: Building Blocks of Kit Applications","text":""},{"location":"omniverse_kit/#understanding-extensions","title":"Understanding Extensions","text":"<p>What is an Extension?</p> <p>Extensions are isolated units of application functionality that serve as the fundamental building blocks of Omniverse Kit-based applications. They can be developed using either: - Python - For rapid development and scripting - C++ - For performance-critical functionality</p> <p>How Extensions Work:</p> <p>When an application starts up, it: 1. Reads the .kit file - References the list of required extensions 2. Loads Extensions - Initializes each extension as part of the application startup 3. Integrates Functionality - Extensions contribute to the application's appearance and capabilities</p> <p>Each template includes several baseline extensions in its <code>.kit</code> file, with the expectation that developers will add additional extensions to build custom functionality.</p>"},{"location":"omniverse_kit/#working-with-extensions-in-development-mode","title":"Working with Extensions in Development Mode","text":"<p>Enabling Developer Mode:</p> <p>To access extension management tools, launch your application with the <code>-d</code> parameter:</p> <p>USD Explorer with Developer Mode:</p> <p>Windows:</p> <pre><code>.\\repo.bat launch -d\n</code></pre> <p>Linux/macOS:</p> <pre><code>./repo.sh launch -d\n</code></pre> <p>Developer Mode Benefits: - Extensions Window - Access to extension browser and management - Developer Menu - Additional development tools and options - Runtime Extension Management - Enable/disable extensions without rebuilding</p>"},{"location":"omniverse_kit/#extension-management-workflow","title":"Extension Management Workflow","text":"<p>Method 1: Runtime Extension Management (USD Explorer)</p> <ol> <li>Launch with Developer Mode: Use the <code>-d</code> parameter when launching</li> <li>Access Extensions Window: Navigate to Window &gt; Extensions</li> <li>Search and Install: Use the search bar to find extensions (e.g., \"measure\")</li> <li>Enable Extensions: Toggle extensions on/off as needed</li> </ol> <p>Example: Adding the Measure Tool 1. Open the Extensions window 2. Search for \"measure\" in the search bar 3. Install and enable the Measure Tool extension 4. Access new functionality through the application interface</p> <p>Method 2: .kit File Configuration (Kit Base Editor)</p> <p>For Kit Base Editor applications, extensions are typically managed through direct <code>.kit</code> file modification:</p> <ol> <li>Launch with Developer Mode:</li> </ol> <p>Windows: <code>bash    .\\repo.bat launch -d</code></p> <p>Linux/macOS: <code>bash    ./repo.sh launch -d</code></p> <ol> <li>Interface Differences:</li> <li>No Tools Menu - Kit Base Editor has a simpler interface</li> <li>Developer Menu Available - Accessible due to <code>-d</code> parameter</li> <li>Extension Management - Primarily through <code>.kit</code> file editing</li> </ol>"},{"location":"omniverse_kit/#extension-development-best-practices","title":"Extension Development Best Practices","text":"<p>Planning Your Extensions: - Modular Design - Keep extensions focused on specific functionality - Dependency Management - Clearly define extension dependencies - User Interface - Design intuitive interfaces for extension features - Performance - Consider the impact on application startup and runtime</p> <p>Integration Strategies: - Template Selection - Choose the appropriate base template for your needs - Extension Layering - Build complex functionality through multiple cooperating extensions - Configuration Management - Use <code>.kit</code> files effectively to manage extension combinations</p>"},{"location":"openusd_applied/","title":"OpenUSD Applied Concepts","text":"<p>Build on foundational knowledge to explore advanced topics in OpenUSD, preparing developers and practitioners to use Universal Scene Description effectively in practical workflows.</p>"},{"location":"openusd_applied/#course-1-learn-openusd-creating-composition-arcs","title":"Course 1: Learn OpenUSD: Creating Composition Arcs","text":"<p>This course covers advanced USD composition techniques and practical implementation strategies:</p> <ul> <li>Layers and Sublayers: Organize and structure USD scene data effectively</li> <li>References and Payloads: Build modular and efficient USD scenes</li> <li>Encapsulation Techniques: Apply proper data organization methods</li> <li>Variant Sets: Create flexible asset variations within USD scenes</li> <li>Inherits and Specializes: Create hierarchical relationships and optimize scene composition</li> <li>LIVRPS Strength Ordering: Predict and control USD composition results</li> <li>Debugging: Troubleshoot complex USD compositions using usdview and Python</li> </ul>"},{"location":"openusd_applied/#setup-and-installation","title":"Setup and Installation","text":""},{"location":"openusd_applied/#installing-usdview","title":"Installing usdview","text":"<p>Documentation: usdview Quickstart Guide</p> <p>usdview is compatible with Linux, macOS, and Windows. These instructions are primarily for Windows and Linux users. macOS users can run usdview but must build it from the OpenUSD repository.</p>"},{"location":"openusd_applied/#download-and-setup","title":"Download and Setup","text":"<ol> <li>Visit NVIDIA's OpenUSD developer resources page</li> <li>Download Pre-Built OpenUSD Libraries and Tools (USD 25.05, Python 3.11)</li> <li>Extract the downloaded archive (this may take several minutes)</li> <li>Rename the extracted folder to <code>usd_root</code></li> </ol>"},{"location":"openusd_applied/#linux-prerequisites","title":"Linux Prerequisites","text":"<p>For Ubuntu systems, install the required X11 dependencies before proceeding:</p> <pre><code>sudo apt-get install libxkbcommon-x11-0 libxcb-xinerama0 libxcb-image0 \\\n                    libxcb-shape0 libxcb-render-util0 libxcb-icccm4 \\\n                    libxcb-keysyms1\n</code></pre> <p>Note: Tested on Ubuntu 22.04 LTS and 24.04 LTS. Adjust the command for other Linux distributions as needed.</p>"},{"location":"openusd_applied/#running-usdview","title":"Running usdview","text":"<p>Navigate to the <code>usd_root/</code> folder in your terminal and run:</p> <p>Windows:</p> <pre><code>.\\scripts\\usdview_gui.bat\n</code></pre> <p>Linux/macOS:</p> <pre><code>./scripts/usdview_gui.sh\n</code></pre> <p>This opens usdview with the HelloWorld.usda file. The <code>usdview_gui</code> script doesn't require a USD file argument, while <code>usdview</code> requires a specific USD file path.</p>"},{"location":"openusd_applied/#adding-to-path-optional","title":"Adding to PATH (Optional)","text":"<p>For convenient access, add the <code>scripts/</code> folder to your PATH environment variable:</p> <p>Windows: 1. Open Start menu \u2192 Type \"SystemPropertiesAdvanced\" \u2192 Press Enter 2. Click \"Environment Variables...\" 3. Double-click the \"Path\" row in User variables 4. Click \"New\" and add the full path to your <code>scripts/</code> folder 5. Test with: <code>usdview_gui.bat</code></p> <p>Linux/macOS:</p> <pre><code>export PATH=&lt;PRE-BUILT BINARIES DIR&gt;/scripts:$PATH\n# Test with:\nusdview_gui.sh\n</code></pre>"},{"location":"openusd_applied/#setting-up-openusd-python-environment","title":"Setting Up OpenUSD Python Environment","text":"<p>The prebuilt OpenUSD binaries include a compatible Python version located in the <code>python/</code> folder.</p>"},{"location":"openusd_applied/#creating-virtual-environment","title":"Creating Virtual Environment","text":"<p>Create a virtual Python environment in the <code>usd_root/</code> directory:</p> <p>Windows:</p> <pre><code>.\\python\\python.exe -m venv .\\pyusd-venv\n</code></pre> <p>Linux/macOS:</p> <pre><code>./python/python -m venv ./pyusd-venv\n</code></pre>"},{"location":"openusd_applied/#activating-the-environment","title":"Activating the Environment","text":"<p>bash/zsh:</p> <pre><code>source pyusd-venv/bin/activate\n</code></pre> <p>&lt;!-- Windows Command Prompt:</p> <pre><code>pyusd-venv\\Scripts\\activate.bat\n``` --&gt;\n\n**Windows PowerShell:**\n```powershell\n(base) PS D:\\IsaacSim\\usd_root&gt; pyusd-venv\\Scripts\\Activate.ps1\n(pyusd-venv) (base) PS D:\\IsaacSim\\usd_root&gt; \n</code></pre> <p>PowerShell Note: You may need to set the execution policy first: <code>powershell Set-ExecutionPolicy RemoteSigned -Scope CurrentUser</code></p>"},{"location":"openusd_applied/#alternative-setup-method","title":"Alternative Setup Method","text":"<p>You can also use the provided environment scripts:</p> <p>Windows:</p> <pre><code>set_usd_env.bat\n</code></pre> <p>Linux/macOS:</p> <pre><code>source set_usd_env.sh\n</code></pre>"},{"location":"openusd_applied/#installing-required-packages","title":"Installing Required Packages","text":""},{"location":"openusd_applied/#usd-core-library","title":"USD Core Library","text":"<p>Install usd-core, which provides access to the USD API:</p> <pre><code>(pyusd-venv) (base) PS D:\\IsaacSim\\usd_root&gt; python -V\nPython 3.11.11\n(pyusd-venv) (base) PS D:\\IsaacSim\\usd_root&gt; pip list            \nPackage    Version\n---------- -------\npip        24.3.1\nsetuptools 70.3.0\n(pyusd-venv) (base) PS D:\\IsaacSim\\usd_root&gt; pip install usd-core\n</code></pre> <p>Verify installation:</p> <pre><code>(pyusd-venv) (base) PS D:\\IsaacSim\\usd_root&gt; python -c \"from pxr import Usd; print(Usd.GetVersion())\"\n(0, 25, 8)\n</code></pre>"},{"location":"openusd_applied/#additional-dependencies","title":"Additional Dependencies","text":"<p>Install Assimp for 3D model processing:</p> <pre><code>pip install assimp-py\n</code></pre> <p>Verify Assimp installation:</p> <pre><code>python -c \"import assimp_py\"\n</code></pre>"},{"location":"openusd_applied/#course-materials","title":"Course Materials","text":"<p>This course includes hands-on activities. Download the required files:</p> <p>\ud83d\udcc1 Course Files Download</p> <p>Setup Instructions: 1. Extract the downloaded archive 2. Copy the <code>composition_arcs</code> folder to your <code>usd_root/</code> directory 3. You can drag and drop it into Visual Studio Code's Explorer panel</p>"},{"location":"openusd_applied/#additional-resources","title":"Additional Resources","text":"<p>Access sample USD content and assets:</p> <p>\ud83c\udfa8 USD Content Samples</p>"},{"location":"openusd_applied/#core-usd-concepts","title":"Core USD Concepts","text":""},{"location":"openusd_applied/#prims-the-foundation-of-usd","title":"Prims: The Foundation of USD","text":"<p>A prim (primitive) is the primary container object in USD that can: - Contain and organize other prims in a hierarchy - Hold various types of data (geometry, materials, transforms, etc.) - Be composed from multiple primSpecs and propertySpecs</p>"},{"location":"openusd_applied/#understanding-specs-and-opinions","title":"Understanding Specs and Opinions","text":"<p>Specs are the building blocks of USD composition: - primSpecs: Define prim-level data and metadata - propertySpecs: Define attribute and relationship data - Opinions: The actual authored values stored in specs within layers</p> <p></p>"},{"location":"openusd_applied/#property-types","title":"Property Types","text":"<p>Properties in USD come in two main forms: - Attributes: Store data values (positions, colors, etc.) - Relationships: Define connections between prims</p> <p>This means propertySpecs include both attributeSpecs and relationshipSpecs.</p>"},{"location":"openusd_applied/#working-with-specs","title":"Working with Specs","text":"<p>Interact with specs using the Sdf (Scene Description Foundations) API: - <code>SdfSpec</code>: Base API for all specs - <code>SdfPrimSpec</code>: API for prim specifications - <code>SdfPropertySpec</code>: API for property specifications</p> <p>Example: In the image above, <code>Sphere</code> is a primSpec, <code>radius</code> is a propertySpec, and <code>1.0</code> is the opinion value.</p>"},{"location":"openusd_applied/#layers-collaborative-scene-composition","title":"Layers: Collaborative Scene Composition","text":""},{"location":"openusd_applied/#what-are-layers","title":"What are Layers?","text":"<p>A layer is a single USD document (file or hosted resource) that contains: - Prims and properties defined as primSpecs and propertySpecs - Sparse definitions that contribute to the final composed scene - Independent data that can be authored by different workstreams</p>"},{"location":"openusd_applied/#collaborative-workflow-benefits","title":"Collaborative Workflow Benefits","text":"<p>Workstreams are different data producers in a project: - Teams, users, departments - Services, applications, tools - Each authors data to separate layers independently</p> <p>This enables USD's core strength: collaborative, non-destructive editing.</p>"},{"location":"openusd_applied/#layer-applications","title":"Layer Applications","text":"<p>Layers are essential for: - Organization: Structure content logically - Optimization: Enable deferred loading (Payloads) - Instancing: Efficient scene reuse - Collaboration: Parallel development workflows</p>"},{"location":"openusd_applied/#sublayers-strength-ordered-composition","title":"Sublayers: Strength-Ordered Composition","text":"<p>Sublayers are ordered lists of USD layers where: - First layer = Strongest opinions - Last layer = Weakest opinions - Each layer overlays on others, providing additional opinions - Contents are included without remapping</p>"},{"location":"openusd_applied/#when-to-use-sublayers","title":"When to Use Sublayers","text":"<p>Sublayers excel in large scene workflows:</p> <p>Example Scenario: - Lighting team \u2192 Works in <code>lighting.usd</code> - Layout team \u2192 Works in <code>layout.usd</code> - Animation team \u2192 Works in <code>animation.usd</code></p> <p>Benefits: - \u2705 Independent, parallel development - \u2705 No blocking between teams - \u2705 Automatic integration of changes - \u2705 Manageable, smaller files</p> <p></p>"},{"location":"openusd_applied/#practical-exercise","title":"Practical Exercise","text":"<p>Explore the composition examples:</p> <pre><code>composition_arcs/sublayers/simple_example/\n\u251c\u2500\u2500 sublayerA.usd\n\u251c\u2500\u2500 sublayerB.usd\n\u2514\u2500\u2500 sublayers_simple.usd\n</code></pre>"},{"location":"openusd_applied/#understanding-over-prims","title":"Understanding \"Over\" Prims","text":"<p>Over (short for \"override\" or \"compose over\"): - Provides neutral containers for overriding opinions - Doesn't change the resolved specifier when stronger than <code>def</code> or <code>class</code> - Used for authoring overriding opinions on existing prims</p> <p>Example: In <code>sublayerB.usda</code>, an <code>over</code> prim provides overriding opinions for the <code>Geometry</code> prim, defining a <code>Sphere</code> within it.</p>"},{"location":"openusd_applied/#advanced-composition-arcs","title":"Advanced Composition Arcs","text":""},{"location":"openusd_applied/#references-and-payloads-asset-reuse-and-performance","title":"References and Payloads: Asset Reuse and Performance","text":"<p>References and payloads are essential composition arcs for managing complex scenes, reusing assets, and optimizing performance in USD workflows.</p>"},{"location":"openusd_applied/#references-modular-asset-composition","title":"References: Modular Asset Composition","text":"<p>What are References?</p> <p>References enable you to create multiple copies of the same content by grafting a prim hierarchy from one layer onto a prim in another layer. This makes them ideal for: - Modularity: Compose smaller asset units into larger aggregates - Reusability: Use the same assets across different scenes - Collaboration: Enable parallel development by multiple contributors</p> <p>Key Characteristics: - Path Translation: Referenced prims undergo name changes and path translations to fit their new namespace - External/Internal: Can target prims from different layers (external) or within the same layer (internal) - Always Loaded: References are always composed and present on the stage</p> <p>Use Case Example: A digital twin of a factory environment can consist of various assets from disparate sources: - Boxes, machinery, conveyor belts, shelving - Each asset referenced multiple times to build the complete factory - Reduces duplication while enabling modular assembly</p> <p>Viewing References in usdview:</p> <p>Windows:</p> <pre><code>.\\scripts\\usdview.bat .\\composition_arcs\\references\\simple_example\\red_cube.usd\n</code></pre> <p>Linux:</p> <pre><code>./scripts/usdview.sh ./composition_arcs/references/simple_example/red_cube.usd\n</code></pre>"},{"location":"openusd_applied/#payloads-on-demand-loading","title":"Payloads: On-Demand Loading","text":"<p>What are Payloads?</p> <p>Payloads are similar to references but with the crucial ability to load and unload content on demand. This enables efficient memory management and faster scene navigation.</p> <p>Key Benefits: - Deferred Loading: Load only the assets you need to inspect - Memory Efficiency: Reduce memory usage by unloading unnecessary content - Performance: Faster scene loading and improved interactivity - Selective Composition: Load specific payloads while keeping others unloaded</p> <p>Workflow Example: 1. Load Stage: Open a city scene with all payloads unloaded (fast loading) 2. Navigate: Browse the scene hierarchy to find areas of interest 3. Selective Loading: Load only the city block or assets you want to inspect 4. Compose: Data from loaded payloads and their dependencies gets composed into the scene</p> <p>Composition Characteristics: - List-Editable: Can be applied to the same prim in lists, like references - Weaker Strength: Lower priority than references in LIVRPS ordering - Recursive Unloading: Unloading a payload also unloads all its descendants</p> <p>Testing Payloads in usdview:</p> <pre><code>.\\scripts\\usdview.bat .\\composition_arcs\\payloads\\simple_example\\red_cube.usd\n</code></pre> <p>Key Observation: When selecting \"Unload\" in usdview, all descendants disappear from the tree view, demonstrating payload's ability to completely remove scene description from composition.</p>"},{"location":"openusd_applied/#encapsulation-asset-self-containment","title":"Encapsulation: Asset Self-Containment","text":"<p>Encapsulation refers to organizing assets in a self-contained manner, ensuring all dependencies are included when referencing or composing assets.</p>"},{"location":"openusd_applied/#understanding-encapsulation","title":"Understanding Encapsulation","text":"<p>What is Encapsulation? - A prim is encapsulated when it is an ancestor of the referenced target prim - When encapsulated, the prim will be referenced along with its ancestor - Promotes modularity and reusability across projects</p>"},{"location":"openusd_applied/#encapsulation-vs-unencapsulation","title":"Encapsulation vs. Unencapsulation","text":"<p>Unencapsulated Assets: - Missing dependencies when referenced - Can break material bindings and relationships - Example: Referencing <code>unencapsulated.usd</code> brings the cube but loses the red material binding</p> <p>Properly Encapsulated Assets: - Include all necessary dependencies within the asset hierarchy - Maintain material bindings and relationships when referenced - Self-contained and portable across different contexts</p>"},{"location":"openusd_applied/#best-practices","title":"Best Practices","text":"<p>Proper Encapsulation Techniques: - Hierarchical Organization: Organize prims within a root prim container - Default Prims: Use default prims as main entry points for assets - Dependency Inclusion: Ensure all materials, textures, and relationships are included - Self-Containment: Assets should work independently when referenced</p>"},{"location":"openusd_applied/#practical-example","title":"Practical Example","text":"<p>Good Encapsulation Example:</p> <pre><code>.\\scripts\\usdview.bat .\\composition_arcs\\references\\encapsulation_example\\encapsulated_GOOD.usd\n</code></pre> <p>Key Benefits: - \u2705 Modularity: Assets work across different projects - \u2705 Reliability: No broken dependencies when referencing - \u2705 Maintainability: Easier to manage and update assets - \u2705 Collaboration: Teams can work with assets independently</p>"},{"location":"openusd_applied/#default-prims","title":"Default Prims","text":"<p>Default prims serve as the main entry point for assets: - Automatically selected when referencing or payloading - Simplify asset usage and composition - Enable consistent asset behavior across different contexts</p>"},{"location":"openusd_applied/#variant-sets-runtime-asset-variations","title":"Variant Sets: Runtime Asset Variations","text":"<p>Variant Sets enable you to define multiple variations or alternative representations of the same asset that can be switched at runtime, providing flexible content management without duplicating assets.</p>"},{"location":"openusd_applied/#what-are-variant-sets","title":"What are Variant Sets?","text":"<p>Core Concept: - Define variations (called \"variants\") of the same dataset - Switch between variations at runtime - Only the selected variant is composed on the stage - All other variants are ignored during traversal</p>"},{"location":"openusd_applied/#common-use-cases","title":"Common Use Cases","text":"<p>Character Outfits Example: - Single character asset with multiple clothing options - Variant set controls which outfit appears in each shot - Eliminates need to manage separate character + clothing combinations - Reduces complexity and asset management overhead</p> <p>Other Applications: - Level of Detail (LOD): High/medium/low resolution variants - Material Variations: Different surface treatments for the same geometry - Seasonal Changes: Summer/winter versions of environments - Configuration Options: Different equipment or accessory combinations</p>"},{"location":"openusd_applied/#how-variant-sets-work","title":"How Variant Sets Work","text":"<p>Variant Set Structure: - Variant Set: Controller that manages variations - Variants: Individual variation options within the set - Selection: Only one variant active at a time - Scope: Can manipulate the prim and any descendant prims</p>"},{"location":"openusd_applied/#variant-capabilities","title":"Variant Capabilities","text":"<p>What Variants Can Do: - Override Properties: Sparsely modify attributes on any descendant prim - Add Geometry: Include additional prims or components - Material Changes: Switch materials, textures, or shading - Transform Modifications: Adjust positions, rotations, or scales</p> <p></p> <p>Example: Two variants override the diffuse color of a cube material: - Blue Variant: Sets material color to blue - Green Variant: Sets material color to green</p>"},{"location":"openusd_applied/#best-practices_1","title":"Best Practices","text":"<p>Variant Set Design Guidelines: - Limited Scope: Keep variant changes predictable and focused - Clear Naming: Use descriptive names for variant sets and variants - Consistent Structure: Maintain similar prim hierarchies across variants - Performance Consideration: Avoid overly complex variant combinations</p>"},{"location":"openusd_applied/#practical-example_1","title":"Practical Example","text":"<p>Testing Variant Sets:</p> <pre><code>.\\scripts\\usdview.bat .\\composition_arcs\\variant_sets\\simple_example\\variant_sets_simple.usd\n</code></pre> <p>In usdview: 1. Select different variants from the variant set dropdown 2. Observe how the scene changes based on the selected variant 3. Note that only one variant is active at any time</p>"},{"location":"openusd_applied/#inherit-and-specialize-arcs-advanced-inheritance-patterns","title":"Inherit and Specialize Arcs: Advanced Inheritance Patterns","text":""},{"location":"openusd_applied/#inherit-arcs-broadcast-inheritance","title":"Inherit Arcs: Broadcast Inheritance","text":"<p>What are Inherit Arcs?</p> <p>Inherit arcs share similarities with programming inheritance, enabling prims to inherit properties and structure from other prims with broadcast capabilities across layer stacks.</p> <p>Key Characteristics: - Source Prim: The prim being inherited from (often uses \"class\" specifier) - Destination Prim: The prim that inherits properties - Broadcast Updates: Changes to source prim propagate to all inheriting prims - Cross-Layer Stack: Works across references and different layer stacks</p> <p>Example: <code>/World</code> prim inherits from <code>/_cube_asset</code>, applying all properties and descendants from the source hierarchy.</p>"},{"location":"openusd_applied/#when-to-use-inherit-arcs","title":"When to Use Inherit Arcs","text":"<p>Ideal Use Cases: - Reusable Assets: Define assets meant to be inherited by multiple prims - Global Updates: Apply changes to all instances of an asset simultaneously - Unencapsulated Sources: Use with unencapsulated inherit source prims for maximum flexibility - Asset Libraries: Create base assets that can be customized per instance</p> <p>Workflow Benefits: 1. Global Namespace: Source prim becomes editable at any stronger layer 2. Broadcast Changes: Modifications propagate to all inheriting prims automatically 3. Individual Overrides: Specific instances can still be customized in stronger layers 4. Cross-Reference: Works across different layer stacks and references</p>"},{"location":"openusd_applied/#class-specifier","title":"Class Specifier","text":"<p>Why Use \"class\" Specifier?</p> <p>When authoring prims meant for inheritance, use the \"class\" specifier:</p> <p>Benefits: - Communication: Clearly indicates the prim is designed for inheritance - Rendering Behavior: Hydra doesn't render \"class\" prims or their descendants - Abstract Data: Perfect for purely abstract/template data - Performance: Avoids unnecessary rendering of template prims</p>"},{"location":"openusd_applied/#specialize-arcs-fallback-inheritance","title":"Specialize Arcs: Fallback Inheritance","text":"<p>What are Specialize Arcs?</p> <p>Specialize arcs provide fallback-based inheritance, similar to traditional object-oriented programming inheritance patterns.</p> <p>Key Differences from Inherit: - Fallback Values: Specs are applied only when no stronger opinion exists - OOP-Style: Behaves like traditional class inheritance - Conditional Application: Only applies if destination prim lacks the specific spec - Weaker Strength: Lowest priority in LIVRPS ordering</p>"},{"location":"openusd_applied/#when-to-use-specialize-arcs","title":"When to Use Specialize Arcs","text":"<p>Traditional OOP Inheritance: - Default Values: Provide fallback values for properties - Class-like Behavior: Objects inherit from classes but can override members - Conditional Updates: Changes to \"class\" only affect objects still using default values - Selective Inheritance: Objects with overrides ignore class updates</p> <p>Behavior Comparison:</p> Aspect Inherit Arcs Specialize Arcs Update Propagation Always broadcasts Only to non-overridden specs Override Behavior Can be overridden in stronger layers Overrides ignore source updates Use Case Global asset updates Traditional OOP inheritance Strength Stronger (\"I\" in LIVRPS) Weakest (\"S\" in LIVRPS)"},{"location":"openusd_applied/#livrps-strength-ordering-composition-resolution-rules","title":"LIVRPS Strength Ordering: Composition Resolution Rules","text":""},{"location":"openusd_applied/#understanding-livrps","title":"Understanding LIVRPS","text":"<p>What is LIVRPS?</p> <p>LIVRPS is a fundamental concept in USD that governs how different composition arcs are applied when creating a scene. This acronym captures all operations used to compose data from layers onto the stage, in the order of their strength.</p> <p>Why Strength Ordering Matters: - Multiple Opinions: USD allows multiple layers and composition operations to define the same property - Conflict Resolution: Strength ordering determines which opinion \"wins\" in the final composed stage - Predictable Results: Ensures consistent and predictable composition behavior - Hierarchical Control: Stronger operations can override weaker ones</p>"},{"location":"openusd_applied/#layer-stack-foundation","title":"Layer Stack Foundation","text":"<p>What is a Layer Stack?</p> <p></p> <p>A layer stack is the ordered set of layers resulting from: 1. Root Layer: The primary layer (strongest) 2. Sublayers: All recursively gathered sublayers 3. Local Composition: Internal references, payloads, variant sets, inherits, and specializes</p> <p>Key Characteristics: - Strength-Ordered: First layer is strongest, subsequent layers are progressively weaker - Recursive Gathering: Includes all nested sublayers automatically - Composition Context: Each layer stack forms a complete composition context - Cross-Stack Operations: Multiple layer stacks are composed using references and payloads</p>"},{"location":"openusd_applied/#livrps-breakdown","title":"LIVRPS Breakdown","text":"<p>The LIVRPS acronym represents both the composition operations and their application order (strongest to weakest):</p> Letter Operation Strength Description L Local Strongest Direct opinions in current layer stack I Inherit Strong Broadcast inheritance from source prims V Variant Medium-Strong Runtime asset variations R Reference Medium Modular asset composition P Payload Medium-Weak On-demand loading composition S Specialize Weakest Fallback inheritance patterns"},{"location":"openusd_applied/#composition-order-rules","title":"Composition Order Rules","text":"<p>Within Each Layer Stack:</p> <ol> <li>Local Opinions (L) - Strongest</li> <li>Direct property values authored in the current layer</li> <li>Immediate overrides and customizations</li> <li> <p>Always take precedence over composition arcs</p> </li> <li> <p>Inherit Arcs (I)</p> </li> <li>Broadcast inheritance from source prims</li> <li>Global updates propagate to all inheriting prims</li> <li> <p>Can be overridden by local opinions</p> </li> <li> <p>Variant Set Arcs (V)</p> </li> <li>Runtime asset variations and configurations</li> <li>Selected variants contribute their opinions</li> <li> <p>Weaker than inherits but stronger than references</p> </li> <li> <p>Reference Arcs (R)</p> </li> <li>Modular asset composition and reuse</li> <li>Brings in external asset hierarchies</li> <li> <p>Can be customized by stronger operations</p> </li> <li> <p>Payload Arcs (P)</p> </li> <li>On-demand loading for heavy assets</li> <li>Similar to references but with deferred loading</li> <li> <p>Slightly weaker than references</p> </li> <li> <p>Specialize Arcs (S) - Weakest</p> </li> <li>Fallback inheritance patterns</li> <li>Only applied when no stronger opinion exists</li> <li>Traditional OOP-style inheritance behavior</li> </ol>"},{"location":"openusd_applied/#recursive-application","title":"Recursive Application","text":"<p>LIVRPS in Practice: - Per Layer Stack: LIVRPS applies within each individual layer stack - Recursive Composition: Each referenced/payload layer stack has its own LIVRPS ordering - Nested Resolution: Composition arcs can contain other composition arcs - Predictable Hierarchy: Maintains consistent strength ordering at all levels</p> <p>Example Scenario:</p> <pre><code>Layer Stack A (Main Scene)\n\u251c\u2500\u2500 Local opinions (strongest)\n\u251c\u2500\u2500 Inherits from /CharacterClass\n\u251c\u2500\u2500 Variant: \"damaged\" selected\n\u251c\u2500\u2500 References: /assets/character.usd\n\u2514\u2500\u2500 Specializes: /defaults/character\n\nLayer Stack B (Referenced Asset)\n\u251c\u2500\u2500 Local opinions in character.usd\n\u251c\u2500\u2500 Inherits from /BaseCharacter\n\u2514\u2500\u2500 References: /assets/geometry.usd\n</code></pre> <p>Resolution Order: 1. Layer Stack A's local opinions 2. Layer Stack A's inherit opinions 3. Layer Stack A's variant opinions 4. Layer Stack B (referenced) - entire LIVRPS hierarchy 5. Layer Stack A's specialize opinions</p>"},{"location":"openusd_applied/#course2-learn-openusd-asset-structure-principles-and-content-aggregation","title":"Course2: Learn OpenUSD: Asset Structure Principles and Content Aggregation","text":"<p>Download the required course files: https://learn.learn.nvidia.com/assets/courseware/v1/b80bc8cc0859b518e050eb259ed5664e/asset-v1:DLI+S-OV-33+V1+type@asset+block/asset-structure-course-files.zip</p> <p>What is an asset? An asset is a named, versioned, and structured container of one or more resources which may include composable OpenUSD layers, textures, volumetric data, and more. You\u2019ll often come across terms like \u201casset\u201d, \u201cmodel\u201d, \u201cassembly\u201d, \u201celement\u201d, \u201ccomponent\u201d, \u201cset\u201d, \u201cshot\u201d, \u201cfile\u201d, and \u201cpackage\u201d when discussing the organization of production data, a product, or a digital twin.</p> <p>Asset structure facilitates reuse of this persistent data.</p> <p>What makes asset structure necessary? Asset structure plays an important role in scaling pipelines and ecosystems. Here\u2019s why:</p> <p>Content flow. Asset structures help map out the journey of content throughout production, ensuring everything runs smoothly. Seamless collaboration. By adopting consistent conventions and patterns, asset structures reduce friction and make it easier for teams to communicate and work together. No one-size-fits-all solution. There isn\u2019t a universally best way to structure an OpenUSD asset, as usage and domains vary. A well-designed asset structure promotes scalability by encouraging parallel and modular workflows, minimizing complexity, and striking a balance between openness and resilience to change.</p> <p>There are four key principles that contribute toward a scalable asset structure:</p> <p>Legibility ensures your asset structure is easy to understand and interpret. Modularity allows for flexibility and reusability. Performance ensures your asset structure is efficient and optimized. Navigability makes it easy for users to find and access the features and properties they need. By adhering to these principles, you can create an asset structure that is not only scalable but also efficient and user-friendly. Let\u2019s dive into each principle to understand more.</p> <p>A legible asset structure should be easy to understand and help new users get up to speed quickly. </p> <p>Each asset designed to be opened as a stage or added to a scene through referencing has a root layer that serves as its foundation. This root layer, known as the asset interface layer, is structured to be the primary means of interacting with the asset.</p> <p>A single asset entry point can typically be specified using the root layer\u2019s defaultPrim metadata. OpenUSD\u2019s composition engine will respect this metadata when referencing. Additionally, different domains (such as renderSettingsPrimPath) may introduce other methods for identifying domain-specific entry points.</p> <p>To improve navigability, it\u2019s common to partition asset structures. Partitioning a hierarchy can prevent unintentional namespace collisions between collaborators and avoid ambiguous semantics. For example, what does it mean for a Sphere to be parented to a Material?</p> <p>Using scope prims is generally the best approach for these organizational primitives, as they don\u2019t have additional semantics like xform does with transform operations.</p> <p>Similarly, it may be useful to group actors and environments under partitioning scopes. This not only aids navigability but also allows users to quickly deactivate all actors or environments by deactivating the root scope.</p> <p>When organizing your prim hierarchy, also consider the following:</p> <p>Naming Conventions: A legible prim hierarchy should promote consistency for better readability. Access Semantics: There are no restrictions on the fields that can be overridden on a prim, so it\u2019s important for collaborators to establish conventions for stable editing. This could include using single or double underscores to discourage users from authoring overrides or to indicate internal use. Embedded Context: Embed context directly into assets to hint to users that these assets are intended to be included by reference.</p> <p>Products developed by organizations rarely consist of a single file. Instead, work is organized into logical, maintainable units. Similarly, assets should model workstreams into layers.</p> <p>User Workstreams: Simple assets can be represented by a single layer, and it may make sense to start there for the sake of simplicity.</p> <p>However, when different tools, users, and departments are responsible for contributing different prims to the final composed asset scene graph (such as geometry and materials), it can be beneficial to split the workstreams into multiple layers.</p> <p>Computational Workstreams: Assets can also be divided into computational workstreams. For instance, a synthetic data simulation might be partitioned across multiple processes or machines. A layer stack can then be used to stitch the results back together.</p> <p>Computational workstreams can be dynamic and may not remain consistent from one evaluation to the next. Consider a layer stack where workloads have been dynamically partitioned across multiple processes.</p> <p>Some workstreams are hybrids, combining both computational and user-driven elements. For example, a layer might contribute synthesized motion on top of a hand-authored initial state created by a user.</p> <p>Asset parameterization enables the reuse of content by allowing certain fields and properties to vary downstream.</p> <p>There are two primary ways to parameterize assets: variant sets and primvars.</p> <p>Variant Sets</p> <p>Primvars are extra parameters that can be interpolated and are primarily used to provide additional data to shading contexts. In OpenUSD, primvars are inherited down the prim hierarchy and can be authored on an ancestor prim, including the entry point of an asset.</p> <p>Instead of expecting users to know whether a complex asset requires payloading, many assets adopt the \u201creference-payload\u201d pattern. This means their interface file is designed to be referenced, with the payload structure internal to the asset.</p> <p>Important and inexpensive fields like variant sets and inherits are elevated above the payload when they are moved from the contents layer to the interface layer.</p> <p>The references to the payload pattern can be used to recast a payload\u2019s opinion ordering strength. While the example above uses an inline payload for brevity, if a mirroring resolver is used, it becomes important to keep the payload contents in separate layers.</p> <p>Through composition, OpenUSD can build complex hierarchies of scenes. At the levels of complexity required for a film production shot or a factory line simulation, a single scenegraph can become difficult to navigate for both algorithms and users.</p> <p>Model hierarchy (also known as kind metadata) offers a separate, higher-level view of the underlying scenegraph.</p> <p>Let\u2019s talk about two of the specific model kinds, components and assemblies.</p> <p>The term component is often overloaded in many domains. It\u2019s helpful to think of component models as roughly corresponding to consumer facing products. For example, a consumer can purchase a pen, and a consumer can purchase a house. Despite their differences in scale and complexity, both of these would be logical component models in a hierarchy.</p> <p>All ancestors of a component model must have their kind metadata set to either a group or a subkind of group, such as assembly. This requirement is primarily to ensure that component discovery is efficient for composition.</p> <p>Since the component model kind denotes a \u201cleaf\u201d asset or pruning point in the model hierarchy, component models cannot contain other component models as descendants. OpenUSD provides the \u201csubcomponent\u201d annotation for important prims that are outside the model hierarchy, which helps facilitate kind-based workflows. Subcomponent prims can contain other subcomponent prims.</p> <p>Assemblies are groups that usually correspond with aggregate assets.</p> <p>Assemblies are important groups that usually correspond to aggregate assets. If a house is a component model, then its neighborhood and city could be assembly models. In this example, a neighborhood may contain multiple intermediate group scopes in between the assembly and component for organizational purposes (say grouping trees, street lights, and architecture separately). Assembly models can contain other assembly models, group kinds, and component models.</p> <p>When creating a model hierarchy, there are four essential factors to keep in mind. These hierarchies should be operational, shallow, consistent, and extensible.</p> <p>Operational: Component and assembly models should be easily referenced in other contexts, and they should include all essential dependencies, such as material bindings or skeleton setups, that downstream users require.</p> <p>Shallowness: Asset structures should encourage a shallow model hierarchy. The kind metadata is explicitly read during composition for all members of the model hierarchy, with costs minimized in a shallow structure. A deep model hierarchy introduces a small but measurable overhead to composition and misses out on performance gains from pruned traversals. A gprim tagged as a component is a sign that a model hierarchy is \u201cdeep\u201d.</p> <p>Consistency: As the language and expectations around components, subcomponents, and assemblies evolve within an ecosystem, it\u2019s crucial that an asset consistently represents one of these concepts. For instance, it\u2019s common to expect component models to be fully packaged and renderable, with their geometry and materials completely specified and organized into Geometry and Materials scopes.</p> <p>Extensibility: The Kind library that ships with OpenUSD can be extended using plugin info, allowing users to define their own extensions to component, assembly, and subcomponent kinds. For example, a pipeline might want to differentiate between various levels of assemblies (such as \u201clocation\u201d vs \u201cworld\u201d) or types of subcomponents.</p>"},{"location":"openusd_applied/#course3-learn-openusd-developing-data-exchange-pipelines","title":"Course3: Learn OpenUSD: Developing Data Exchange Pipelines","text":"<p>OpenUSD is commonly referred to as USD, which stands for Universal Scene Description. </p> <p>OpenUSD was developed by Pixar Animation Studios to address the complex 3D workflow challenges common in assembling and animating virtual worlds, where dozens of studios and specialties have to come together and collaborate on a single 3D scene.</p> <p>To solve these problems, Pixar built USD with a core set of schemas to effectively describe 3D scenes, including meshes, materials, lights, and cameras. These data models describe many of the elements you would need to be able to make a 3D scene, such as those seen in an animated film, and have been used successfully by many studios as a common interchange format to pass the data between applications.</p> <p>There are four common implementations for adding OpenUSD support for your data format or application:</p> <p>Importers Exporters Standalone converters File format plugins At a high-level, whether your data is represented as a file format or runtime format may dictate the type of implementation, but there are other factors to consider as well.</p> <p>File format plugins are a unique feature of OpenUSD. They allow OpenUSD to compose with additional file formats and even non-file-based sources, such as databases and procedurally generated content.</p> <p>OpenUSD\u2019s flexibility and modularity make it suitable for a wide range of applications and industries. These features and its universality make the challenges of data exchange very apparent:</p> <p>Data exchange is typically lossy. Not every data model can be mapped directly. Not every consumer may be interested in all the data exported by a digital content creation (DCC) tool. For example, a CAD application may not need animation data. The content structure exported by a DCC tool may not be suitable for every organization and its workflows. There is no single content structure suitable for all end clients and organizations. Not every client can handle the same content fidelity (e.g., high-quality offline renderer vs. real-time 3D app on a phone).</p> <p>Two-Phase Data Exchange: To help with these data exchange challenges, we propose a two-phase approach: extract and transform. This is loosely inspired by Extract-Transform-Load (ETL) used in data analytics and machine learning. By following this two-phase data exchange approach, your implementation will solve many of today\u2019s data exchange challenges and help you serve the widest range of customers.</p> <p>Data exchange to OpenUSD should start with the extract phase. The goal is to translate the data to OpenUSD as directly as possible to maintain fidelity to the source format. Try to map concepts from the source format to concepts in OpenUSD as much as possible to preserve the integrity and structure of the source data.</p> <p>The transform phase consists of one or more optional steps added to better meet end client and user needs. This includes:</p> <p>Applying user export options. Making changes to the content structure that deviate from the source format. Implementing optimizations for better end client and workflow performance.</p> <p>OpenUSD Exchange SDK helps developers design and develop their own USD I/O solutions that produce consistent and correct USD assets across diverse 3D ecosystems.</p> <p>It provides higher-level convenience functions on top of lower-level USD concepts, so developers can quickly adopt OpenUSD best practices when mapping their native data sources to OpenUSD-legible data models.</p> <p>Data transformation allows customers to tailor data to their specific needs. While DCC developers should ideally handle data extraction, they can leave transformation open for third-party developers to extend.</p> <p>Key aspects of data transformation include:</p> <p>Export options Content re-structuring Optimizations, e.g., mesh merging</p>"},{"location":"openusd_foundations/","title":"OpenUSD Foundations","text":"<p>Developed by Pixar Animation Studios, OpenUSD is an open-source framework for creating, simulating, and collaborating in 3D worlds. OpenUSD serves as the foundational technology for NVIDIA Omniverse.</p> <p>The universal structure of OpenUSD allows developers to integrate multiple assets into a single \"stage,\" enabling dynamic scene manipulation. Each OpenUSD asset contains independent data layer stacks, such as geometry, shading, or textures, which can be interchanged without affecting other elements. For example, in an OpenUSD stage representing a kitchen environment, the data layers for a chair or stove can be swapped independently, enabling rapid scene updates.</p>"},{"location":"openusd_foundations/#important-resources","title":"Important Resources","text":"<ul> <li>OpenUSD Documentation</li> <li>OpenUSD GitHub Repository</li> <li>OpenUSD Learning Path</li> </ul>"},{"location":"openusd_foundations/#learning-path-overview","title":"Learning Path Overview","text":"<p>Learn OpenUSD, the core of Omniverse's scene representation format. The NVIDIA OpenUSD Learning Path contains 7 comprehensive courses covering OpenUSD fundamentals and advanced topics.</p> <p>Install <code>usd-core</code></p> <pre><code>pip install usd-core\npip install usd2gltf #USD to glTF conversion using usd2gltf\nwhich usd2gltf\n</code></pre> <p>Test creation</p> <pre><code>python -c \"from pxr import Usd, UsdGeom; stage = Usd.Stage.CreateInMemory(); cube = UsdGeom.Cube.Define(stage, '/hello'); stage.Export('test_cube.usda'); print('Test USD file created successfully')\"\n</code></pre> <p>usd2gltf conversion</p> <pre><code>(mypy311) kaikailiu@Kaikais-MacBook-Pro omniverselab % usd2gltf --input test_cube.usda --output test_cube.glb &amp;&amp; echo 'USD to glTF conversion successful'\nConverting: test_cube.usda\nTo: test_cube.glb\nConverted!\nUSD to glTF conversion successful\n</code></pre>"},{"location":"openusd_foundations/#course-1-learning-about-stages-prims-and-attributes","title":"Course 1: Learning About Stages, Prims, and Attributes","text":"<p>Course Link</p> <p>This course covers the fundamental concepts of OpenUSD:</p> <ul> <li>Create and manipulate USD files - Learn to set up USD files from scratch, establishing the foundation for 3D scenes</li> <li>Define primitives - Gain hands-on experience with defining various types of prims, the building blocks of USD</li> <li>Establish scene hierarchies - Organize and structure 3D elements effectively for coherent and manageable scenes</li> <li>Add dynamic lighting - Implement lighting systems to bring scenes to life with enhanced visual appeal</li> <li>Manage attributes and metadata - Master the details of setting, getting, and manipulating essential scene elements</li> <li>Traverse and inspect USD files - Develop skills to navigate through USD files and understand scene details</li> <li>Verify prim existence - Learn techniques to check for specific prims, ensuring scene integrity and completeness</li> </ul>"},{"location":"openusd_foundations/#stage","title":"Stage","text":"<p>An OpenUSD stage represents the scenegraph, which defines the contents and structure of a scene. It consists of a hierarchy of objects called prims, which can represent geometry, materials, lights, and other organizational elements. The scene is stored as a data structure of connected nodes, hence the term \"scenegraph.\"</p> <p>A stage can be composed of: - A single USD file (e.g., a robot asset) - Multiple USD files combined together (e.g., a factory containing many robot assets)</p> <p>The stage represents the composed result of all contributing files or layers. Composition is the algorithm that determines how USD files (or layers) should be assembled and combined into the final scene.</p> <p>Benefits of OpenUSD stages:</p> <ul> <li>Modularity - Enable modification of individual elements without altering original files (\"non-destructive\" editing)</li> <li>Scalability - Efficiently manage large datasets through features like payloads and composition</li> </ul> <pre><code># Create a new, empty USD stage where 3D scenes are assembled\nUsd.Stage.CreateNew()\n\n# Open an existing USD file as a stage\nUsd.Stage.Open()\n\n# Saves all layers in a USD stage\nUsd.Stage.Save()\n</code></pre>"},{"location":"openusd_foundations/#hydra-rendering-architecture","title":"Hydra Rendering Architecture","text":"<p>Hydra is a powerful rendering architecture within OpenUSD that enables efficient and flexible rendering of complex 3D scenes. It serves as a bridge between scene description data (USD) and rendering backends (OpenGL, DirectX, Metal, Vulkan).</p> <p>Key Features:</p> <ul> <li>Extensible Architecture - Supports multiple renderers including Arnold, RenderMan, and others</li> <li>Built-in Renderers - Ships with HdStorm (real-time renderer), HdTiny, and HdEmbree</li> <li>Render Delegates - Plugin system for custom rendering backends</li> <li>Wide Adoption - Used by usdview and many other USD-based tools</li> </ul> <p>HdStorm Renderer: The included real-time OpenGL/Metal/Vulkan render delegate that provides out-of-the-box visualization capabilities for developers.</p> <p>Programming Interface: Interaction with Hydra is typically done through the HydraPython API, which provides programmatic control over the rendering process.</p>"},{"location":"openusd_foundations/#usd-file-formats","title":"USD File Formats","text":"<p>OpenUSD supports four core file formats for storing and exchanging 3D scene data, including meshes, cameras, lights, and shaders. All formats can be accessed through Python bindings.</p>"},{"location":"openusd_foundations/#usda-usda-ascii-format","title":"USDA (.usda) - ASCII Format","text":"<ul> <li>Human-readable ASCII text files encoding scene descriptions</li> <li>Editable - Can be manually edited and inspected</li> <li>Use case - Optimal for small files and stages referencing external content</li> <li>Benefits - Easy debugging and version control</li> </ul>"},{"location":"openusd_foundations/#usdc-usdc-binary-crate-format","title":"USDC (.usdc) - Binary Crate Format","text":"<ul> <li>Compressed binary format for efficient storage</li> <li>Performance optimized - Minimizes load times through compression and memory mapping</li> <li>Use case - Ideal for numerically-heavy data like geometry</li> <li>Benefits - Faster file access and reduced file sizes</li> </ul>"},{"location":"openusd_foundations/#usd-usd-flexible-format","title":"USD (.usd) - Flexible Format","text":"<ul> <li>Format agnostic - Can be either ASCII or binary</li> <li>Interchangeable - Format can be changed without breaking references</li> <li>Debugging friendly - Binary assets can be converted to ASCII for inspection</li> </ul>"},{"location":"openusd_foundations/#usdz-usdz-archive-format","title":"USDZ (.usdz) - Archive Format","text":"<ul> <li>Packaged delivery - Uncompressed ZIP archive containing all necessary assets</li> <li>Distribution ready - Ideal for shipping complete, finalized assets</li> <li>Limitation - Not suitable for assets still under development</li> </ul>"},{"location":"openusd_foundations/#prim-primitives","title":"Prim (Primitives)","text":"<p>Primitives, or prims, are the fundamental building blocks of any OpenUSD scene. Understanding prims is essential for anyone working with 3D content creation and manipulation in the OpenUSD ecosystem.</p> <p>What is a Prim? A prim is a container that holds various types of data, attributes, and relationships which define an object or entity within a scene. Prims can represent: - Imageable entities - Meshes, lights, cameras - Non-imageable entities - Materials, transforms (xforms), organizational elements</p> <p>Hierarchical Structure: Prims are organized in a hierarchical structure, creating a scenegraph that represents relationships and transformations between objects.</p> <p>Prim Paths: Each prim has a unique identifier called a path for location within the scene graph.</p> <p>Example: <code>/World/BuildingA/Geometry/building_geo</code> - <code>building_geo</code> is a child of <code>Geometry</code> - <code>Geometry</code> is a child of <code>BuildingA</code> - <code>BuildingA</code> is a child of the root <code>World</code></p> <p>Python API for Prims:</p> <pre><code># Generic USD API command. Used to define a new prim on a stage at a specified path, and optionally the type of prim.\nstage.DefinePrim(path, prim_type)\n\n# Specific to UsdGeom schema. Used to define a new prim on a USD stage at a specified path of type Xform. \nUsdGeom.Xform.Define(stage, path)\n\n# Retrieves the children of a prim. Useful for navigating through the scenegraph.\nprim.GetChildren()\n\n# Returns the type of the prim, helping identify what kind of data the prim contains.\nprim.GetTypeName()\n\n# Returns all properties of the prim.\nprim.GetProperties()\n</code></pre>"},{"location":"openusd_foundations/#scope","title":"Scope","text":"<p>In OpenUSD, a scope is a special type of prim used primarily as a grouping mechanism in the scenegraph. It serves as an organizational container without representing any geometry or renderable content.</p> <p>Key Characteristics: - Organizational tool - Acts like a folder for organizing related prims - Non-transformable - Cannot be transformed, promoting lightweight usage - Logical grouping - Useful for grouping materials, animation, or geometry prims</p> <p>Use Cases: - Grouping all material-related prims - Organizing animation elements - Structuring geometry hierarchies</p> <p>Python API for Scope:</p> <pre><code># Used to define a new scope at a specified path on a given stage\nUsdGeom.Scope.Define(stage, path)\n\n# This command is generic, but it's useful to confirm that a prim's type is a scope, ensuring correct usage in scripts\nprim.IsA(UsdGeom.Scope)\n</code></pre>"},{"location":"openusd_foundations/#xform-transform","title":"Xform (Transform)","text":"<p>In OpenUSD, an xform is a type of prim that stores transformation data (translation, rotation, scaling) which applies to its child prims. Xforms are powerful tools for grouping and manipulating the spatial arrangement of objects in 3D scenes.</p> <p>Purpose: - Spatial transformation - Defines the coordinate space for child prims - Hierarchical transforms - Transformations cascade down to children - Scene organization - Groups related objects under common transforms</p> <p>Transform Operations: Xforms support multiple transformation operations that can be combined in specific orders. The order of operations is crucial as different sequences yield different results.</p> <p>Python API for Xform:</p> <pre><code># Used to define a new Xform prim at a specified path on a given stage\nUsdGeom.Xform.Define(stage, path)\n\n# Retrieves the order of transformation operations, which is crucial for understanding how multiple transformations are combined. Different orders can yield different results, so understanding XformOpOrder is important. \nxform.GetXformOpOrderAttr()\n\n# Adds a new transform operation to the xform prim, such as translation or rotation, with specified value   \nxform.AddXformOp(opType, value)\n</code></pre>"},{"location":"openusd_foundations/#usd-modules","title":"USD Modules","text":"<p>The USD code repository consists of four core packages: base, usd, imaging, and usdImaging. For basic USD authoring and reading, you only need the base and usd packages.</p> <p>Core Modules:</p>"},{"location":"openusd_foundations/#usd-module","title":"Usd Module","text":"<ul> <li>Purpose - Core client-facing module for authoring, composing, and reading USD</li> <li>Functionality - Interface for creating/opening stages and interacting with prims, properties, metadata, and composition arcs</li> </ul>"},{"location":"openusd_foundations/#sdf-module-scene-description-foundation","title":"Sdf Module (Scene Description Foundation)","text":"<ul> <li>Purpose - Foundations for serializing scene description to text-based file format</li> <li>Functionality - Implements scene description layers (SdfLayer) and manages prim/property paths</li> </ul>"},{"location":"openusd_foundations/#gf-module-graphics-foundation","title":"Gf Module (Graphics Foundation)","text":"<ul> <li>Purpose - Foundation classes and functions for graphics operations</li> <li>Functionality - Linear algebra, mathematical operations, basic geometry, and 3D data types</li> </ul> <p>Schema Modules: Schemas are grouped into domains, each with its own module: - UsdGeom - Geometry data - UsdShade - Materials and shaders - UsdLux - Lighting - UsdPhysics - Physics scene description</p> <p>Python Import:</p> <pre><code># Import Usd, Sdf, and Gf libraries from Pixar\nfrom pxr import Usd, Sdf, Gf\n</code></pre> <p>UsdLux Example:</p> <pre><code># Import the UsdLux module\nfrom pxr import UsdLux\n\n# Create a sphere light primitive\nUsdLux.SphereLight.Define(stage, '/path/to/light')\n\n# Set the intensity of a light primitive\nlight_prim.GetIntensityAttr().Set(500)\n</code></pre>"},{"location":"openusd_foundations/#usd-properties-attributes","title":"USD Properties: Attributes","text":"<p>USD attributes are one of two types of USD properties (the other being relationships). Properties describe the characteristics of prims within a USD scene.</p> <p>What are Attributes? - Data storage - Store values that define appearance, behavior, or other prim properties - Schema-specific - Each schema provides specific APIs for accessing its attributes - Typed values - Support various data types (floats, vectors, colors, etc.)</p> <p>Working with Attributes: Generally use schema-specific APIs rather than generic property access.</p> <pre><code># Get the radius value of sphere_prim that is of type UsdGeom.Sphere\nsphere_prim.GetRadiusAttr().Get()\n\n# Set the double-sided property of the prim\nsphere_prim.GetDoubleSidedAttr().Set(True)\n</code></pre>"},{"location":"openusd_foundations/#usd-properties-relationships","title":"USD Properties: Relationships","text":"<p>Relationships establish connections between prims, acting as pointers or links between objects in the scene hierarchy. They enable prims to target or reference other prims, attributes, or relationships, establishing dependencies between scenegraph objects.</p> <p>Key Functions: - Connect prims - Link objects in the scene hierarchy - Reference data - Point to attributes or other relationships - Establish dependencies - Create data flow between scene elements</p> <pre><code># Get the target paths of a relationship\nUsdRelationship.GetTargets()\n\n# Set the target paths for a relationship\nUsdRelationship.SetTargets()\n\n# Add a new target path to a relationship\nUsdRelationship.AddTarget()\n\n# Remove a target path from a relationship\nUsdRelationship.RemoveTarget()\n</code></pre>"},{"location":"openusd_foundations/#primvars-primitive-variables","title":"Primvars (Primitive Variables)","text":"<p>Primvars are special attributes that enable efficient management and manipulation of hierarchical object data in complex 3D scenes. They address key computer graphics challenges:</p> <p>Problems Solved: - Shader binding - Bind user data on geometric primitives for shader access during rendering - Surface interpolation - Specify values associated with vertices/faces that interpolate across primitive surfaces - Data inheritance - Inherit attributes down namespace hierarchies for sparse authoring</p> <p>Python API for Primvars:</p> <pre><code># Constructs a UsdGeomPrimvarsAPI on UsdPrim prim\nprimvar_api = UsdGeom.PrimvarsAPI(prim)\n\n# Creates a new primvar called displayColor of type Color3f[]\nprimvar_api.CreatePrimvar('displayColor', Sdf.ValueTypeNames.Color3fArray)\n\n# Gets the displayColor primvar\nprimvar = primvar_api.GetPrimvar('displayColor')\n\n# Sets displayColor values\nprimvar.Set([Gf.Vec3f(0.0, 1.0, 0.0)])\n\n# Gets displayColor values\nvalues = primvar.Get()\n</code></pre>"},{"location":"openusd_foundations/#xformcommonapi","title":"XformCommonAPI","text":"<p>XformCommonAPI is a component of the OpenUSD framework that facilitates authoring and retrieval of common transformation operations. This API provides a single interface for translation, rotation, scale, and pivot operations that maintains compatibility with import/export workflows across various tools.</p> <p>Purpose: - Simplified transforms - Common transformation interface - Tool compatibility - Compatible with import/export across many tools - Standardized interchange - Simplifies transformation data exchange</p>"},{"location":"openusd_foundations/#stage-traversal","title":"Stage Traversal","text":"<p>Stage traversal enables efficient navigation and manipulation of the scenegraph. You can iterate through child prims, access parent prims, and traverse hierarchies to find specific prims of interest.</p> <p>Key Concepts: - Navigation - Move through the scene hierarchy programmatically - Filtering - Use predicates to filter traversal results - Efficiency - Optimized algorithms for large scene graphs</p> <p>Traversal Methods: Traversal works via the <code>Usd.PrimRange</code> class, with <code>stage.Traverse()</code> as a common convenience method.</p> <pre><code># Open a USD file and create a Stage object\nstage = Usd.Stage.Open('car.usda')\n\n# Traverses the stage of prims that are active\nstage.Traverse()\n\n# Define a predicate to filter prims that are active and loaded\npredicate = Usd.PrimIsActive &amp; Usd.PrimIsLoaded\n\n# Traverse starting from the given prim and based on the predicate for filtering the traversal\nUsd.PrimRange(prim, predicate=predicate)\n</code></pre>"},{"location":"openusd_foundations/#course-2-working-with-prims-and-default-schemas","title":"Course 2: Working With Prims and Default Schemas","text":"<p>Course Link</p>"},{"location":"openusd_foundations/#specifiers","title":"Specifiers","text":"<p>Specifiers in OpenUSD convey the intent for how a prim or primSpec should be interpreted in the composed scene. There are three types of specifiers:</p> <ul> <li>Def (Define) - Defines the prim in the current layer, indicating the prim exists and is available for processing</li> <li>Over - Provides overrides or additional data for an existing prim</li> <li>Class - Defines a class that can be inherited by other prims</li> </ul> <pre><code># Get a prim\u2019s specifier\nprim.GetSpecifier()\n\n# Set a prim\u2019s specifier\nprim.SetSpecifier(specifier)\n\n</code></pre> <p>Every prim has a specifier. To make a prim present on the stage and available for processing, you define (<code>def</code>) that prim. You can use override specifiers (<code>over</code>) to hold opinions that will be applied to prims in another layer, enabling non-destructive editing workflows. Class specifiers (<code>class</code>) can be used to set up a collection of opinions and properties to be composed by other prims.</p>"},{"location":"openusd_foundations/#prim-paths","title":"Prim Paths","text":"<p>In OpenUSD, a path represents the location of a prim within a scene hierarchy. The string representation consists of prim names separated by forward slashes (<code>/</code>), similar to file system paths.</p> <p>Path Structure: - Root - Represented by a forward slash (<code>/</code>) - Hierarchy - Each level separated by <code>/</code> - Example - <code>/World/Geometry/Box</code> represents:   - <code>Box</code> is a child of <code>Geometry</code>   - <code>Geometry</code> is a child of <code>World</code>   - <code>World</code> is a child of the root</p> <pre><code># Import the Sdf class\nfrom pxr import Sdf\n\n# Return the path of a Usd.Prim as an Sdf.Path object\nUsd.Prim.GetPath()\n\n# Retrieve a Usd.Prim at the specified path from the Stage\nUsd.Stage.GetPrimAtPath()\n</code></pre>"},{"location":"openusd_foundations/#default-prim","title":"Default Prim","text":"<p>A default prim is a top-level prim that serves as the primary entry point for a stage. It's part of the scene's metadata and acts as the \"control point\" that helps tools and applications know where to start.</p> <p>Importance: - Best practice - Should be set on all stages - Tool compatibility - Required by many tools and applications - Validation - <code>usdchecker</code> reports errors if not set - Referencing - Eliminates need to specify target prim when referencing stages</p> <pre><code>from pxr import Usd, UsdGeom, Sdf\n\n# Create a new USD stage\nstage = Usd.Stage.CreateInMemory()\n\n# Define a top-level Xform prim\ndefault_prim = UsdGeom.Xform.Define(stage, Sdf.Path(\"/World\")).GetPrim()\n\n# Set the Xform prim as the default prim\nstage.SetDefaultPrim(default_prim)\n\n# Export the stage to a string to verify\nusda = stage.GetRootLayer().ExportToString()\nprint(usda)\n\n# Check that the expected default prim was set\nassert stage.GetDefaultPrim() == default_prim\n</code></pre>"},{"location":"openusd_foundations/#schemas","title":"Schemas","text":"<p>Schemas give meaning to prims in OpenUSD by defining \"What is this element? What capabilities does it have?\" They define data models and APIs for encoding and interchanging 3D and non-3D concepts through OpenUSD.</p> <p>Purpose: - Data models - Define structure and behavior of scene elements - Blueprints - Provide templates for authoring and retrieving data - Interoperability - Ensure consistent data interpretation across tools - Extensibility - Allow custom schemas for specialized use cases</p> <p>Schema Types: - Typed schemas - Define specific prim types (e.g., UsdGeom.Mesh) - API schemas - Add functionality to existing prims (e.g., UsdGeom.PrimvarsAPI)</p> <pre><code># Retrieve the schema info for a registered schema\nUsd.SchemaRegistry.FindSchemaInfo()\n\n# Retrieve the schema typeName\nUsd.SchemaRegistry.GetSchemaTypeName()\n\n</code></pre>"},{"location":"openusd_foundations/#isa-schemas-typed-schemas","title":"IsA Schemas (Typed Schemas)","text":"<p>There are two main types of schemas in OpenUSD: IsA schemas and API schemas.</p> <p>IsA schemas, also known as Typed schemas or Prim schemas, define what a prim fundamentally is. Each prim can only have one IsA schema at a time, assigned through the <code>typeName</code> metadata.</p> <p>IsA schemas are derived from the core class <code>UsdTyped</code>, the base class for all typed schemas. They can be:</p> <ul> <li>Concrete schemas - Can be instantiated as prims in USD scenes (e.g., <code>UsdGeomMesh</code>, <code>UsdGeomScope</code>). They provide both a name and a typeName in their definition.</li> <li>Abstract schemas - Serve as base classes for related concrete schemas (e.g., <code>UsdGeomPointBased</code> serves as a base for geometric objects containing points like meshes and curves). They provide a name but no typeName.</li> </ul> <p>Common Schema Libraries:</p> <p>UsdGeom defines schemas for representing geometric objects, such as meshes, cameras, and curves. It also includes schemas for transformations, visibility, and other common properties.</p> <pre><code># Import related classes\nfrom pxr import UsdGeom\n\n# Define a sphere in the stage\nsphere = UsdGeom.Sphere.Define(stage, \"/World/Sphere\")\n\n# Get and Set the radius attribute of the sphere\nsphere.GetRadiusAttr().Set(10)\n</code></pre> <p>UsdLux defines schemas for representing light sources in a scene, including sphere lights, disk lights, and distant lights.</p> <p>Examples include <code>UsdLuxDiskLight</code>, <code>UsdLuxRectLight</code>, and <code>UsdLuxSphereLight</code>.</p> <pre><code># Import related classes\nfrom pxr import UsdLux\n\n# Define a disk light in the stage\ndisk_light = UsdLux.DiskLight.Define(stage, \"/World/Lights/DiskLight\")\n\n# Get all Attribute names that are a part of the DiskLight schema\ndl_attribute_names = disk_light.GetSchemaAttributeNames()\n\n# Get and Set the intensity attribute of the disk light prim\ndisk_light.GetIntensityAttr().Set(1000)\n</code></pre>"},{"location":"openusd_foundations/#api-schemas","title":"API Schemas","text":"<p>API schemas are similar to IsA schemas but do not specify a <code>typeName</code>, making them non-concrete. They add functionality to existing prims without changing their fundamental type.</p> <p>Key Characteristics: - Named with the suffix \"API\" (e.g., <code>UsdShadeConnectableAPI</code>) - Properties are namespaced with the schema's base name (e.g., <code>UsdPhysics.RigidBodyAPI.CreateVelocityAttr()</code> creates <code>physics:velocity</code>) - Can be single-apply (applied once per prim) or multiple-apply (applied multiple times with different instance names) - Listed in the <code>apiSchemas</code> metadata and queryable via the <code>HasAPI()</code> method - Applied to already-typed prims to add specialized behaviors</p> <p>Example: UsdPhysicsRigidBodyAPI adds physics properties to any <code>UsdGeomXformable</code> object for rigid body dynamics simulation.</p> <pre><code># Import related classes\nfrom pxr import UsdPhysics\n\n# Apply a UsdPhysics Rigidbody API on the cube prim\ncube_rb_api = UsdPhysics.RigidBodyAPI.Apply(cube.GetPrim())\n\n# Get the Kinematic Enabled Attribute \ncube_rb_api.GetKinematicEnabledAttr()\n\n# Create a linear velocity attribute of value 5\ncube_rb_api.CreateVelocityAttr(5)\n</code></pre>"},{"location":"openusd_foundations/#course-3-learn-openusd-using-attributes","title":"Course 3: Learn OpenUSD: Using Attributes","text":"<p>Course Link</p>"},{"location":"openusd_foundations/#retrieving-attributes","title":"Retrieving Attributes","text":"<pre><code># Get the property names of the cube prim\ncube_prop_names = cube.GetPrim().GetPropertyNames()\n</code></pre>"},{"location":"openusd_foundations/#value-resolution","title":"Value Resolution","text":"<p>Value resolution is the algorithm by which final values for properties or metadata are compiled from all sources. The algorithm processes an ordered list of values including:</p> <ul> <li>Default values - Schema-defined defaults</li> <li>Time samples - Animated values at specific time codes</li> <li>Fallback values - Values from composition layers</li> </ul> <p>The algorithm returns the resolved value based on composition strength ordering and time sampling rules.</p> <p>Value resolution allows OpenUSD to provide a rich set of composition semantics while keeping the core lightweight and performant for random access to composed data.</p>"},{"location":"openusd_foundations/#metadata","title":"Metadata","text":"<p>Metadata in OpenUSD refers to name-value pairs that provide additional, non-animatable information attached to prims or their properties. It allows you to add custom information or annotations to scene description elements without modifying the underlying schema or data model.</p> <p>Metadata is stored separately from the primary data and can be accessed and modified independently. It's typically used to store supplementary information not directly related to the geometry or rendering of an object.</p> <p>Key Differences Between Metadata and Attributes: - Schema integration - Metadata is separate from the core schema, while attributes are part of the schema definition - Purpose - Metadata stores supplementary information, while attributes store data directly related to object properties or behavior - Time sampling - Metadata cannot be sampled over time (no timesamples), making it more efficient to evaluate and store than attribute values</p> <pre><code># Retrieve the metadata value associated with the given key for a USD Object\nusdobject.GetMetadata('key')\n\n# Set the metadata value for the given key on a USD Object\nusdobject.SetMetadata('key', value)\n\n# Retrieve the metadata value associated with the given key for the stage\nstage.GetMetadata('key')\n\n# Set the metadata value for the given key on the stage\nstage.SetMetadata('key', value)\n\n# Use for better performance if accessing a single value and not all the metadata within a key\nGetMetadataByDictKey()\n</code></pre>"},{"location":"openusd_foundations/#custom-attributes","title":"Custom Attributes","text":"<p>Custom attributes in OpenUSD are user-defined properties that can be added to prims to store additional data. Unlike schema attributes, which are predefined and standardized, custom attributes allow users to extend OpenUSD's functionality at runtime to suit specific requirements.</p> <p>Common Use Cases: - Metadata storage - Additional information about a prim (author names, creation dates, custom tags) - Animation data - Custom animation curves or parameters not covered by standard schema attributes - Simulation parameters - Parameters for physics simulations or procedural generation processes - Arbitrary end user data - Runtime-defined custom data for specialized workflows</p> <pre><code>stage = Usd.Stage.CreateInMemory()\nprim = stage.DefinePrim(\"/ExamplePrim\", \"Xform\")\nserial_num_attr = prim.CreateAttribute(\"serial_number\", Sdf.ValueTypeNames.String)\n\nassert serial_num_attr.IsCustom()\n\nmtce_date_attr = prim.CreateAttribute(\"maintenance_date\", Sdf.ValueTypeNames.String)\nserial_num_attr.Set(\"qt6hfg23\")\nmtce_date_attr.Set(\"20241004\")\n\nprint(f\"Serial Number: {serial_num_attr.Get()}\")\nprint(f\"Last Maintenance Date: {mtce_date_attr.Get()}\")\n</code></pre>"},{"location":"openusd_foundations/#course-4-learn-openusd-traversing-stages","title":"Course 4: Learn OpenUSD: Traversing Stages","text":"<p>Stage traversal is the process of navigating through a stage's scenegraph to query or edit scene data. You can traverse the scenegraph by iterating through child prims, accessing parent prims, and navigating the hierarchy to find specific prims of interest.</p>"},{"location":"openusd_foundations/#traversal-methods","title":"Traversal Methods","text":"<p>Stage traversal operates through the <code>Usd.PrimRange</code> class. Other methods like <code>stage.Traverse()</code> use <code>Usd.PrimRange</code> as their foundation.</p> <pre><code># Open a USD file and create a Stage object\nstage = Usd.Stage.Open('car.usda') \n\n# Traverses the stage of prims that are active\nstage.Traverse() \n\n# Define a predicate to filter prims that are active and loaded\npredicate = Usd.PrimIsActive &amp; Usd.PrimIsLoaded\n\n# Traverse starting from the given prim and based on the predicate for filtering the traversal\nUsd.PrimRange(prim, predicate=predicate)\n\n</code></pre>"},{"location":"openusd_foundations/#active-vs-inactive-prims","title":"Active vs. Inactive Prims","text":"<p>In OpenUSD, all prims are active by default. Making a prim inactive models a non-destructive deletion from a stage. Deactivating a prim provides a way to temporarily remove (or prune) prims and their descendants from composition and processing, which can make traversals more efficient.</p> <p>Active prims and their active children are visited and processed during stage traversals and operations. However, making a prim inactive by setting its \"active\" metadata to <code>false</code> prevents that prim from being visited and prevents its descendant prims from being composed onto the stage.</p> <pre><code># Make the prim at /Parent inactive\nstage.GetPrimAtPath('/Parent').SetActive(False)\n\n</code></pre>"},{"location":"openusd_foundations/#python-api-for-active-state","title":"Python API for Active State","text":"<p>Use these functions to manage prim active state:</p> <ul> <li><code>UsdPrim.SetActive(bool)</code> - Set the \"active\" metadata for a prim</li> <li><code>UsdPrim.IsActive()</code> - Return whether a prim is currently active on the stage</li> </ul>"},{"location":"openusd_foundations/#course-5-learn-openusd-understanding-model-kinds","title":"Course 5: Learn OpenUSD: Understanding Model Kinds","text":"<p>Model kinds are metadata that organize and categorize different types of scene elements or prims into a hierarchical structure. Understanding kinds enables the creation of modular, reusable assets and can significantly impact how well you manage complex 3D scenes.</p>"},{"location":"openusd_foundations/#kind-categories","title":"Kind Categories","text":"<p>Kinds are predefined categories that define the role and behavior of different prims within the scene hierarchy. The main kinds include:</p> <ul> <li>Component - Self-contained, referenceable assets</li> <li>Group - Organizational units for related models</li> <li>Assembly - Containers for combining parts into larger entities</li> </ul> <p>The base class for group and component kinds is Model, which should not be assigned as any prim's kind.</p>"},{"location":"openusd_foundations/#component","title":"Component","text":"<p>A component is a reusable, self-contained asset that is complete and referenceable. Think of component models as consumer-facing products like a pen or a house. While drastically different in scale, both would be logical component models in a hierarchy.</p>"},{"location":"openusd_foundations/#subcomponent","title":"Subcomponent","text":"<p>A component cannot contain other component models as descendants, which is why we have subcomponents. Subcomponents aren't model kinds in the traditional sense, but they identify important prims within a component.</p>"},{"location":"openusd_foundations/#groups-and-assemblies","title":"Groups and Assemblies","text":"<p>All parents of a component model must have their kind metadata set to group or assembly:</p> <ul> <li>Group - An organizational unit used to logically group related models together</li> <li>Assembly - A subkind of group that serves as a container for combining various parts or assets into larger, more complex entities</li> </ul> <p>For example, if a house is your component, the neighborhood or city might be assembly models containing multiple group scopes, such as trees and street lights.</p>"},{"location":"openusd_foundations/#model-hierarchy","title":"Model Hierarchy","text":"<p>Prims of the group, assembly, and component kind (and any custom kind inheriting from them) make up the model hierarchy. This hierarchy enables better asset organization in your scene, facilitating navigation, asset management, and high-level reasoning about scene structure.</p>"},{"location":"openusd_foundations/#python-api-for-model-kinds","title":"Python API for Model Kinds","text":"<pre><code># Construct a Usd.ModelAPI on a prim\nprim_model_api = Usd.ModelAPI(prim)\n\n# Return the kind of a prim\nprim_model_api.GetKind()\n\n# Set the kind of a prim\nprim_model_api.SetKind(kind) \n\n# Return \"true\" if the prim represents a model based on its kind metadata\nprim_model_api.IsModel()  \n</code></pre> <p>Model kinds in OpenUSD provide a structured way to organize and manage complex 3D scenes. By defining and adhering to these kinds, artists, designers, and developers can create modular, reusable assets that can be easily combined, referenced, and shared across different projects and workflows.</p>"},{"location":"openusd_foundations/#course-6-learn-openusd-setting-up-basic-animations","title":"Course 6: Learn OpenUSD: Setting Up Basic Animations","text":"<p>Code: <code>openusd/01_Learn_OpenUSD_Setting_Up_Basic_Animations.ipynb</code></p> <p>In OpenUSD, <code>timeCode</code> and <code>timeSample</code> are two important concepts that enable working with animations and simulation in USD scenes.</p>"},{"location":"openusd_foundations/#timecode","title":"TimeCode","text":"<p>TimeCode is a point in time with no unit assigned to it. You can think of these as frames whose units are derived from the stage.</p>"},{"location":"openusd_foundations/#timesample","title":"TimeSample","text":"<p>TimeSample refers to the individual time-varying values associated with an attribute in USD. Each attribute can have a collection of timeSamples that map timeCode to the attribute's data type values, allowing for animation over time.</p>"},{"location":"openusd_foundations/#time-scaling-and-interpolation","title":"Time Scaling and Interpolation","text":"<p>In a USD scene, the timeCode ordinates of all timeSamples are scaled to seconds based on the <code>timeCodesPerSecond</code> metadata value defined in the root layer. This allows flexibility in encoding timeSamples within a range and scale suitable for the application, while maintaining a robust mapping to real-world time for playback and decoding.</p> <p>For example, if the root layer has <code>timeCodesPerSecond=24</code>, a timeCode value of <code>48.0</code> would correspond to 2 seconds (48/24) of real time after timeCode 0.</p> <p>TimeSamples store time-varying data for attributes, such as positions, rotations, or material properties. When an attribute is evaluated at a specific timeCode, the value is linearly interpolated from the surrounding timeSamples, allowing for smooth animation playback.</p>"},{"location":"openusd_foundations/#python-api-for-timesamples","title":"Python API for TimeSamples","text":"<pre><code># Returns authored TimeSamples\ncube.GetDisplayColorAttr().GetTimeSamples()\n\n# Sets TimeSample Value (Gf.Vec3d(0,-4.5,0)) at a specified TimeCode (30)\nsphere_xform_api.SetTranslate(Gf.Vec3d(0,-4.5,0), time=Usd.TimeCode(30))\n\n</code></pre>"},{"location":"openusd_foundations/#course-7-learn-openusd-an-introduction-to-strength-ordering","title":"Course 7: Learn OpenUSD: An Introduction to Strength Ordering","text":"<p>Code:  - <code>openusd/01_Learn_OpenUSD_Intro_to_Strength_Ordering.ipynb</code> - <code>openusd/02_Learn_OpenUSD_Experimenting_With_VariantSets.ipynb</code></p>"},{"location":"openusd_foundations/#layers-and-composition","title":"Layers and Composition","text":"<p>OpenUSD scenes are organized into layers, whose contents can be combined or overridden to create a composed scene. Each layer contains a subset of the complete scene data (such as geometry, materials, or animations).</p> <p>This layered approach enables non-destructive editing, where changes made to one layer do not affect the others. Layers can be thought of as modular pieces that, when composed together, form a complete scene.</p> <p>A layer is a single file or resource that contains scene description data. This could be: - A USD file (<code>.usd</code>, <code>.usda</code>, <code>.usdc</code>) - A file format supported by a plugin (e.g., <code>.gltf</code>, <code>.fbx</code>) - A resource that's not file-based at all (e.g., from a database)</p> <p>Each USD stage is made of layer stacks composed at runtime to represent a scene.</p> <p>While Photoshop composes its final product by compiling each layer on top of the one below it, USD's composition engine combines the data across composition arcs according to a specific strength ordering, resolving conflicts based on the arcs' relative strengths. This strength ordering is referred to as LIVRPS (pronounced \"liver peas\") \u2013 an acronym we'll explain below.</p>"},{"location":"openusd_foundations/#layer-usage","title":"Layer Usage","text":"<p>Layers are used for: - Separating scene data by discipline for parallel and modular workstreams - Creating reusable asset libraries that can be referenced across multiple scenes - Enabling collaborative workflows where different teams or artists can work on separate layers concurrently - Structuring scene data for efficient loading and instancing (such as using payloads or references) - Versioning and non-destructive editing by introducing new layers for changes</p>"},{"location":"openusd_foundations/#common-layer-functions","title":"Common Layer Functions","text":"<p>The following are common functions we use when interacting with layers in USD:</p> <pre><code>layer.Reload()  # Clears all content/opinions not saved on that layer\nlayer.Save()    # Saves content from that layer to disk\n</code></pre>"},{"location":"openusd_foundations/#understanding-strength-ordering","title":"Understanding Strength Ordering","text":"<p>Strength ordering governs how opinions and namespaces from multiple layers (or scene sub-hierarchies) are combined, and how conflicts are resolved during scene composition. From the composition, the opinions will be ordered based on their strength and the strongest opinion will take priority.</p> <p>Strength ordering is the ordered list of composition arcs that determines the precedence of data during composition. When conflicting opinions (data values) exist across layers, the stronger layer's opinion takes precedence, allowing for non-destructive overrides and layering of scene data.</p> <p>During scene composition, USD's composition engine builds a graph of the layers in the specified strength order. That order is affectionately referred to as the acronym LIVRPS, which stands for the list of composition operations, ordered from strongest to weakest: Local, Inherits, VariantSets, References, Payloads and Specializes.</p>"},{"location":"openusd_foundations/#livrps-breakdown-strongest-to-weakest","title":"LIVRPS Breakdown (Strongest to Weakest)","text":""},{"location":"openusd_foundations/#1-local-l","title":"1. Local (L)","text":"<p>The algorithm iterates through the local opinions. Local opinions are any opinion authored directly on a layer or a sublayer of a layer, without any additional composition.</p>"},{"location":"openusd_foundations/#2-inherits-i","title":"2. Inherits (I)","text":"<p>Inherits allows opinions authored on one source prim in the scenegraph to affect all prims in the scenegraph that author an inherits arc to that source prim. For example, you can make changes to all pine trees in a forest without changing the source of the pine tree itself.</p>"},{"location":"openusd_foundations/#3-variant-sets-v","title":"3. Variant Sets (V)","text":"<p>Variant sets define one or more scenegraph hierarchies for a prim (called variants), and compose one of them. For example, an object can have multiple geometric representations.</p>"},{"location":"openusd_foundations/#4-references-r-and-payloads-p","title":"4. References (R) and Payloads (P)","text":"<p>References compose the contents of a separate layer as a scenegraph. Payloads are similar, but contain the ability to load or unload the layer from the stage at runtime. A typical use of references and payloads would be to bring props into an environment, e.g., furniture in a room.</p>"},{"location":"openusd_foundations/#5-specializes-s","title":"5. Specializes (S)","text":"<p>Specializes is essentially authoring a new fallback value for a property; so if all the other compositional choices result in no value, the specializes value will win. This is commonly used with material libraries - for example, a basic Plastic material may be specialized by a RoughPlastic material which reduces the value on the glossiness property. Any subsequent opinion on the RoughPlastic material will take precedence, because specializes is the weakest composition arc.</p>"},{"location":"openusd_foundations/#composition-process","title":"Composition Process","text":"<p>For each prim and property, the engine evaluates the opinions from the layers according to LIVRPS, giving precedence to the stronger layer's opinion when conflicts arise. Stronger layers can override or add to the data defined in weaker layers, enabling non-destructive editing and overrides. LIVRPS is applied recursively - for example, when composing a reference, local opinions within the reference are strongest, followed by inherits, followed by variant sets, etc.</p> <p>The final composed scene, what we refer to as the USD stage, represents the combined data from all layers, with conflicts resolved according to the strength ordering.</p>"},{"location":"openusd_foundations/#references-in-detail","title":"References in Detail","text":"<p>References in Universal Scene Description are a composition arc that enable the composition of prims and their descendants onto other prims \u2013 this allows us to use references to aggregate larger scenes from smaller units of scene description. This can be done with: - External references: Load data from other files - Internal references: Load data from other parts of the hierarchy</p> <p>They are fundamental in USD\u2019s composition system, enabling modular and reusable scene description, and they are the second most important composition arc in USD, after sublayers.</p> <p>A reference statement includes: - The address of the layer to reference from (can be omitted for internal references) - The prim path to reference (can be omitted if you want to load an entire external layer which has a default prim defined)</p> <p>When a prim is composed via a reference arc, USD first composes the layer stack of the referenced prim, then adds the resulting prim spec to the destination prim. Then, it applies any overrides or additional composition arcs from the destination prim.</p>"},{"location":"openusd_foundations/#python-api-for-references","title":"Python API for References","text":"<pre><code># Return a UsdReferences object for managing references on a prim\nprim.GetReferences()\n\n# Add a reference to the specified asset and prim path\nreferences.AddReference(assetPath, primPath) \n\n# Remove all references from a prim\nreferences.ClearReferences()\n</code></pre>"},{"location":"openusd_foundations/#common-use-cases","title":"Common Use Cases","text":"<p>References are useful for: - Building large, complex scenes by referencing smaller sub-scenes or components - Creating asset libraries where assets, materials, or other props are reused across several scenes - Enabling modular workflows that support collaborative development and asset management</p>"},{"location":"ros2_integration/","title":"ROS 2 Integration","text":"<p>Bridge Isaac Sim with ROS 2.</p>"}]}